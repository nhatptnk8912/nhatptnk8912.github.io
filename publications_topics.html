<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><br /></div>
<div class="menu-category">Nhat Ho</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publications_type.html">Publications&nbsp;(by&nbsp;type)</a></div>
<div class="menu-item"><a href="publications_topics.html">Publications&nbsp;(by&nbsp;topic)</a></div>
<div class="menu-item"><a href="collaborators.html">Students/&nbsp;Collaborators</a></div>
<div class="menu-item"><a href="workshops.html">Workshops/&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<p><br /></p>
<h1>Publications (by topics)</h1>
<p>(<font color=red size=-0.3><b>* = equal contribution </b></font>)
<br /> 
(<font color=red size=-0.3><b>** = alphabetical order </b></font>)
<br />
(<font color=red size=-0.3><b> Each paper appears in a single topic </b></font>)
<br /></p>
<h2 style="color:steelblue;">Graphical models / Hierarchical models / Mixture models / Bayesian nonparametrics</h2>
<hr>
<ul>
<li><p><a href="Mixture_Experts_Arxiv.pdf"><font color=blue size=+0.3> Convergence rates for Gaussian mixtures of experts</font></a>. <i>Under revision</i>.
<br />
<b>Nhat Ho</b>, Chiao-Yu Yang, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1901.05078.pdf"><font color=blue size=+0.3> On posterior contraction of parameters and interpretability in Bayesian mixture modeling </font></a>. <i> Bernoulli 27 (4), 2159-2188, 2021</i>.
<br />
Aritra Guha, <b>Nhat Ho</b>, XuanLong Nguyen. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.00704.pdf"><font color=blue size=+0.3> Uniform convergence rates for maximum likelihood estimation under two-component Gaussian mixture models</font></a>. <i>Arxiv Preprint; Longer version to be submitted</i>. 
<br />
Tudor Manole<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>. </p>
</li>
</ul>
<ul>
<li><p><a href="DPMM_Extension_Work.pdf"><font color=blue size=+0.3> On posterior inference for the number of Clusters in Dirichlet process mixture models</font></a>. <i>Under review</i>. 
<br />
Chiao-Yu Yang, Eric Xia, <b>Nhat Ho</b>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="Weak_Separation_AISTATS.pdf"><font color=blue size=+0.3> Weak separation in mixture models and implications for principal stratification</font></a>. <i>AISTATS, 2022</i>.
<br /> 
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Avi Feller<font color=red size=-0.3><b>**</b></font>, Evan Greif<font color=red size=-0.3><b>**</b></font>, Luke W. Miratrix<font color=red size=-0.3><b>**</b></font>, Natesh S.
Pillai<font color=red size=-0.3><b>**</b></font>. </p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Revisiting convergence rate of penalized MLE on finite mixture models</font></a>. <i>To be submitted</i>. 
<br />
Tudor Manole<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>.
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Bayesian sieves for unbounded parameter space andexcess mass contraction under misspecification </font>. <i>To be submitted</i>.
<br />
Aritra Guha, <b>Nhat Ho</b>, Chiao-Yu Yang, Michael I. Jordan, Long Nguyen.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Statistical efficiency of parameter estimation in generalized contaminated models</font>. <i>To be submitted</i>.
<br />
Dat Do<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Long Nguyen.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1609.02655.pdf"><font color=blue size=+0.3> Singularity structures and impacts on parameter estimation behavior in finite mixtures of distributions</font></a>. <i>SIAM Journal on Mathematics of Data Science (SIMODS), 1(4), 730â€“758, 2019</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1709.08094.pdf"><font color=blue size=+0.3> Robust estimation of mixing measures in finite mixture models</font></a>. <i>Bernoulli, 26(2), 828-857, 2020</i>.
<br />
<b>Nhat Ho</b>, XuanLong Nguyen, Ya'acov Ritov. </p>
</li>
</ul>
<ul>
<li><p><a href="AoS_2016.pdf"><font color=blue size=+0.3>Convergence rates of parameter estimation for some weakly identifiable finite mixtures</font></a>. <i>Annals of Statistics, 44(6), 2726-2755, 2016</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen. </p>
</li>
</ul>
<ul>
<li><p><a href="Ejs_2016.pdf"><font color=blue size=+0.3>On strong identifiability and convergence rates of
parameter estimation in finite mixtures</font></a>. <i>Electronic Journal of Statistics, 10(1), 271-307, 2016</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen. </p>
</li>
</ul>
<h2 style="color:steelblue;">Optimal transport (Computation and methods)</h2>
<hr>
<ul>
<li><p><a href="https://arxiv.org/pdf/1906.01437.pdf"><font color=blue size=+0.3> On the efficiency of the Sinkhorn and Greenkhorn algorithms and their acceleration for optimal transport</font></a>. <i>Under revision</i>. 
<br />
Tianyi Lin, <b>Nhat Ho</b>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1910.00152.pdf"><font color=blue size=+0.3> On the complexity of approximating multi-marginal optimal transport</font></a>. <i>Journal of Machine Learning Research (JMLR), 2022</i>. 
<br />
Tianyi Lin, <b>Nhat Ho</b>, Marco Cuturi, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2108.07992.pdf"><font color=blue size=+0.3>  On multimarginal partial optimal transport: Equivalent forms and computational complexity</font></a>. <i> AISTATS, 2022</i>. 
<br />
Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Khang Le<font color=red size=-0.3><b>*</b></font>, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2108.09645.pdf"><font color=blue size=+0.3>  An efficient mini-batch method via partial transportation</font></a>. <i>Under review</i>. 
<br />
Khai Nguyen, Dang Nguyen, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2108.10961.pdf"><font color=blue size=+0.3>  Entropic Gromov-Wasserstein between Gaussian distributions</font></a>. <i>Under review</i>. 
<br />
Khang Le<font color=red size=-0.3><b>*</b></font>, Dung Le<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Dat Do, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2102.05912.pdf"><font color=blue size=+0.3> BoMb-OT: On batch of mini-batches optimal transport </font></a>. <i>Under review</i>.
<br />
Khai Nguyen, Dang Nguyen, Quoc Nguyen, Tung Pham, Dinh Phung, Hung Bui, Trung Le, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2102.06857.pdf"><font color=blue size=+0.3> On robust optimal transport: Computational complexity and barycenter computation </font></a>. <i>Advances in NeurIPS, 2021</i>.
<br />
Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Khang Le<font color=red size=-0.3><b>*</b></font>, Quang Nguyen, Tung Pham, Hung Bui<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2002.04783.pdf"><font color=blue size=+0.3> Fixed-support Wasserstein barycenters:
computational hardness and fast algorithm </font></a>. <i>Advances in NeurIPS, 2020</i>. 
<br />
Tianyi Lin, <b>Nhat Ho</b>, Xi Chen, Marco Cuturi, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.07458.pdf"><font color=blue size=+0.3> Projection robust Wasserstein distance and Riemannian optimization </font></a>. <i>Advances in NeurIPS, 2020</i>. 
<br />
Tianyi Lin<font color=red size=-0.3><b>*</b></font>, Chenyou Fan<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>, Marco Cuturi, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p><a href="Tree_Gromov_AISTATS.pdf"><font color=blue size=+0.3> Flow-based alignment approaches
for probability measures in different spaces</font></a>. <i>AISTATS, 2021</i>. 
<br />
Tam Le<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Makoto Yamada.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1910.04483.pdf"><font color=blue size=+0.3> On scalable variant of Wasserstein barycenter </font></a>. <i>Under review</i>. 
<br />
Tam Le<font color=red size=-0.3><b>*</b></font>, Viet Huynh<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Dinh Phung, Makoto Yamada.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2002.03293.pdf"><font color=blue size=+0.3> On unbalanced optimal transport: an analysis of Sinkhorn algorithm</font></a>. <i>Proceedings of the ICML, 2020</i>. 
<br />
Khiem Pham<font color=red size=-0.3><b>*</b></font>, Khang Le<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>, Tung Pham, Hung Bui. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1905.09952.pdf"><font color=blue size=+0.3> Fast algorithms for computational optimal transport and Wasserstein barycenter</font></a>. <i>AISTATS, 2020</i>. 
<br />
Wenshuo Guo, <b>Nhat Ho</b>, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1901.06482.pdf"><font color=blue size=+0.3> On efficient optimal transport: an analysis of greedy and accelerated mirror descent algorithms</font></a>. <i>Proceedings of the ICML, 2019</i>.
<br />
Tianyi Lin<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1810.11911.pdf"><font color=blue size=+0.3> Probabilistic multilevel clustering via composite transportation distance</font></a>. <i>AISTATS, 2019</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Viet Huynh<font color=red size=-0.3><b>*</b></font>, Dinh Phung, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1706.03883.pdf"><font color=blue size=+0.3> Multilevel clustering via Wasserstein means</font></a>. <i>Proceedings of the ICML, 2017</i>.
<br />
<b>Nhat Ho</b>, XuanLong Nguyen, Mikhail Yurochkin, Hung Hai Bui, Viet Huynh, and Dinh Phung.
<br /> <br />
<i>Journal extension</i>: <a href="https://arxiv.org/pdf/1909.08787.pdf"><font color=blue size=+0.3> On efficient multilevel clustering via Wasserstein distances</font></a>. <i>Journal of Machine Learning Research (JMLR), 22, 1-43, 2021</i>.</p>
</li>
</ul>
<h2 style="color:steelblue;">Optimization in statistical settings / Distributed computing</h2>
<hr>
<ul>
<li><p><a href="https://arxiv.org/pdf/2005.11411.pdf"><font color=blue size=+0.3> Instability, statistical inference and computational efficiency </font></a>. <i>Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2102.02756.pdf"><font color=blue size=+0.3> On the computational and statistical complexity of over-parameterized matrix sensing </font></a>. <i>Under review</i>.
<br />
Jiacheng Zhuo, Jeongyeol Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p><a href="Polyak_Step_Size_V3.pdf"><font color=blue size=+0.3> Towards statistical and computational complexities of Polyak step size gradient descent </font></a>. <i>AISTATS, 2022</i>. 
<br />
Tongzheng Ren<font color=red size=-0.3><b>*</b></font>, Fuheng Cui<font color=red size=-0.3><b>*</b></font>, Alexia Atsidakou<font color=red size=-0.3><b>*</b></font>, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Normalized_GD.pdf"<font color=blue size=+0.3> Improving computational complexity in statistical models with second-order information </font></a>. <i> Under review</i>. 
<br />
Tongzheng Ren, Jiacheng Zhuo, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.06448.pdf"><font color=blue size=+0.3> Probabilistic best subset selection via gradient-based optimization</font></a>. <i>Under review</i>. 
<br />
Mingzhang Yin, <b>Nhat Ho</b>, Bowei Yan, Xiaoning Qian, Mingyuan Zhou.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.02601.pdf"><font color=blue size=+0.3> On the minimax optimality of the EM algorithm for learning two-component mixed linear regression</font></a>. <i>AISTATS, 2021</i>. 
<br />
Jeong Y. Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p><a href="Arxiv_TF.pdf"><font color=blue size=+0.3> On structured filtering-clustering: Global error bound and optimal first-order algorithms</font></a>. <i> AISTATS, 2022</i>. 
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Tianyi Lin<font color=red size=-0.3><b>*</b></font>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1810.00828.pdf"><font color=blue size=+0.3> Singularity,
misspecification, and the convergence rate of EM</font></a>. <i>Annals of Statistics, 48(6), 3161-3182, 2020</i>. 
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1902.00194.pdf"><font color=blue size=+0.3> Sharp analysis of Expectation-Maximization for weakly identifiable models</font></a>. <i>AISTATS, 2020</i>.
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu. </p>
</li>
</ul>
<ul>
<li><p><a href="EM_misspecified_unified.pdf"><font color=blue size=+0.3> Theoretical guarantees for EM under misspecified Gaussian mixture models </font></a>. <i>Advances in NeurIPS 31, 2018</i>.
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan.   </p>
</li>
</ul>
<h2 style="color:steelblue;">Sampling and Markov chains / (Approximate) Bayesian inference</h2>
<hr>
<ul>
<li><p><a href="Bayesian_Consistency.pdf"><font color=blue size=+0.3> Bayesian consistency with the supremum metric </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="SDE_Arxiv_submission.pdf"><font color=blue size=+0.3> A diffusion process perspective on the posterior contraction rates for parameters</font></a>. <i>Under revision</i>.
<br />
Wenlong Mou<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1912.05153.pdf"><font color=blue size=+0.3> Sampling for Bayesian mixture models: MCMC with polynomial-time mixing</font></a>. <i>Under revision</i>.
<br />
Wenlong Mou, <b>Nhat Ho</b>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> A partial differential equation perspective on Berstein-Von Mises theorem</font>. <i>To be submitted</i>.
<br />
Wenlong Mou, <b>Nhat Ho</b>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan.</p>
</li>
</ul>
<h2 style="color:steelblue;"> Nonparametric estimation</h2>
<hr>
<ul>
<li><p><a href="JMLR_V1.pdf"><font color=blue size=+0.3> Multivariate smoothing via the Fourier integral theorem and Fourier kernel </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="Statistical_Fourier_Theorem.pdf"><font color=blue size=+0.3> Statistical analysis using the Fourier integral theorem </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="main_version.pdf"><font color=blue size=+0.3> On integral theorems: Monte Carlo estimators and optimal functions </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="BONuS__Arxiv_.pdf"><font color=blue size=+0.3> BONuS: Multiple multivariate testing with a data-adaptive test statistic </font></a>. <i>Under review</i>.
<br />
Chiao-Yu Yang, Lihua Lei, <b>Nhat Ho</b>, Will Fithian.</p>
</li>
</ul>
<h2 style="color:steelblue;">Deep learning (Generative models, Transformer, Transfer learning, Bayesian neural networks, etc.)</h2>
<hr>
<ul>
<li><p><font color=blue size=+0.3> Projection optimization for minibatch optimal transport</font></a>. <i> To be submitted</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Revisiting projected Wasserstein metric on images</font></a>. <i> To be submitted</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>	
<ul>
<li><p><font color=blue size=+0.3> Revisiting Transformer from a geometric perspective </font></a>. <i> To be submitted</i>. 
<br />
Tan Nguyen, Khai Nguyen, Richard Baraniuk, Stanley Osher<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="Transformer_mixture_key.pdf"<font color=blue size=+0.3> Efficient Transformer with shared keys </font></a>. <i>Under review</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Dung Le, Khuong Nguyen, Anh Tran, Richard Baraniuk, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Stanley Osher<font color=red size=-0.3><b>*</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="LAMDA_ICML.pdf"><font color=blue size=+0.3> LAMDA: Label matching deep domain adaptation</font></a>. <i>Proceedings of the ICML, 2021</i>.
<br />
Trung Le, Tuan Nguyen, <b>Nhat Ho</b>, Hung Bui, Dinh Phung.  </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2002.07367.pdf"><font color=blue size=+0.3> Distributional sliced-Wasserstein and applications to deep generative modeling</font></a>. <i>ICLR, 2021</i>.
<br />
Khai Nguyen, <b>Nhat Ho</b>, Tung Pham, Hung Bui. </p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Point-set distances for learning representations of 3D point clouds</font></a>. <i>International Conference on Computer Vision (ICCV), 2021</i>.
<br />
Trung Nguyen, Hieu Pham, Tam Le, <b>Nhat Ho</b>, Tung Pham, Son Hua.  </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2102.07927.pdf"><font color=blue size=+0.3> Structured Dropout variational inference for Bayesian neural networks </font></a>. <i>Advances in NeurIPS, 2021</i>.
<br />
Son Nguyen<font color=red size=-0.3><b>*</b></font>, Duong Nguyen<font color=red size=-0.3><b>*</b></font>, Khai Nguyen, Khoat Than, Hung Bui<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2010.01787.pdf"><font color=blue size=+0.3> Improving relational regularized autoencoders with spherical sliced fused Gromov Wasserstein</font></a>. <i>ICLR, 2021</i>.
<br />
Khai Nguyen, Son Nguyen, <b>Nhat Ho</b>, Tung Pham, Hung Bui.  </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1811.02657.pdf"><font color=blue size=+0.3> A Bayesian perspective of convolutional neural networks through a deconvolutional generative model</font></a>. <i>Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Ankit Patel, Anima Anandkumar, Michael I. Jordan, Richard Baraniuk. </p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> An efficient method for sharing in Transformer </font></a>. <i> Under review</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Khai Nguyen, Hai Do, Richard Baraniuk, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>,Stanley Osher<font color=red size=-0.3><b>*</b></font>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> A novel perspective to prunning in Transformer </font></a>. <i> Under review</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Khai Nguyen, Richard Baraniuk, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>,Stanley Osher<font color=red size=-0.3><b>*</b></font>.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2020-08-10 20:31:45 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
