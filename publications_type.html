<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><br /></div>
<div class="menu-category">Nhat Ho</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publications_type.html">Publications&nbsp;(by&nbsp;type)</a></div>
<div class="menu-item"><a href="publications_topics.html">Publications&nbsp;(by&nbsp;topic)</a></div>
<div class="menu-item"><a href="publications_year.html">Publications&nbsp;(by&nbsp;year)</a></div>
<div class="menu-item"><a href="collaborators.html">Students/&nbsp;Collaborators</a></div>
<div class="menu-item"><a href="workshops.html">Workshops/&nbsp;Talks/&nbsp;Lectures</a></div>
</td>
<td id="layout-content">
<p><br /></p>
<h1>Preprints/ Publications (by journal/ conference)</h1>
<p>(<font color=red size=-0.3><b>* = equal contribution </b></font>)
<br /> 
(<font color=red size=-0.3><b>** = alphabetical order </b></font>)
<br />
(<font color=red size=-0.3><b><span>&dagger;</span> = co-last author </b></font>)
<br /></p>
<h2 style="color:steelblue;">Journal Submissions</h2>
<hr>
<ul>
<li><p><a href="Statistical_Fourier_Theorem.pdf"><font color=blue size=+0.3> Generative models from the multivariate Fourier integral theorem </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> A general theory for softmax gating Gaussian mixture of
experts </font>. <i> Under review</i>.
<br />
Huy Nguyen, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Alessandro Rinaldo<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> A model-independent approach for federated learning in heterogeneous and label-scarce environments </font>. <i> Under review</i>.
<br />
Disha Makhija, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Joydeep Ghosh<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Energy meets sliced Wasserstein distance: From linear to nonlinear projections </font>. <i> To be submitted</i>.
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.00704.pdf"><font color=blue size=+0.3> Uniform convergence rates for maximum likelihood estimation under two-component Gaussian mixture models</font></a>. <i>Arxiv Preprint</i>. 
<br />
Tudor Manole, <b>Nhat Ho</b>.
<br /> <br />
<i>Journal extension</i>:  <font color=blue size=+0.3> Minimax optimal convergence rates for location-scale Gaussian mixture models</font></a>. <i>To be submitted</i>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> On mean-field variational inference: Non-asymptotic approximation, Bayesian inference, and parameter estimation</font>. <i>To be submitted</i>.
<br />
Wenlong Mou, <b>Nhat Ho</b>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="BONuS__Arxiv_.pdf"><font color=blue size=+0.3> BONuS: Multiple multivariate testing with a data-adaptive test statistic </font></a>. <i>Arxiv Preprint</i>.
<br />
Chiao-Yu Yang, Lihua Lei, <b>Nhat Ho</b>, Will Fithian.</p>
</li>
</ul>
<h2 style="color:steelblue;">Conference Submissions</h2>
<hr>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.02935"><font color=blue size=+0.3> On expert estimation in hierarchical mixture of experts: Beyond softmax gating functions </font></a>. <i> Under review</i>. 
<br />
Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Xing Han<font color=red size=-0.3><b>*</b></font>, Carl Harris, Suchi Saria, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.11222"><font color=blue size=+0.3> Quadratic gating functions in mixture of experts: A statistical insight </font></a>. <i> Under review</i>. 
<br />
Pedram Akbarian<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Xing Han<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.08117"><font color=blue size=+0.3> On barycenter computation: Semi-unbalanced optimal transport-based method on Gaussians </font></a>. <i> Under review</i>. 
<br />
Hai Nguyen<font color=red size=-0.3><b>*</b></font>, Dung Le<font color=red size=-0.3><b>*</b></font>, Phi Nguyen, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.02615"><font color=blue size=+0.3> LoGra-Med: Long context multi-graph alignment for medical vision-language model </font></a>. <i> Under review</i>. 
<br />
Duy M. H. Nguyen, Nghiem T. Diep, Trung Q. Nguyen, Hoang-Bao Le, Tai Nguyen, Tien Nguyen, TrungTin Nguyen, <b>Nhat Ho</b>, Pengtao Xie, Roger Wattenhofer, James Zhou, Daniel Sonntag, Mathias Niepert.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.12258"><font color=blue size=+0.3> ACUS: Audio captioning with unbiased sliced Wasserstein kernel </font></a>. <i> Under review</i>. 
<br />
Manh Luong, Khai Nguyen, Dinh Phung, <b>Nhat Ho</b>, Reza Haf, Lizhen Qu.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.04196"><font color=blue size=+0.3> Improving generalization with flat Hilbert Bayesian inference </font></a>. <i> Under review</i>. 
<br />
Tuan Truong, Quyen Tran, Ngoc-Quan Pham, Dinh Phung, <b>Nhat Ho</b>, Trung Le.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.04327"><font color=blue size=+0.3> Leveraging hierarchical taxonomies in prompt-based continual learning </font></a>. <i> Under review</i>. 
<br />
Quyen Tran, Minh Le, Tuan Truong, Dinh Phung, Linh Ngo, Thien Nguyen, <b>Nhat Ho</b>, Trung Le.</p>
</li>
</ul>
<ul>
<li><p><a href="HDP_optimization.pdf"><font color=blue size=+0.3> Borrowing strength in distributionally robust optimization via hierarchical Dirichlet processes </font></a>. <i> Under review</i>. 
<br />
Nicola Bariletto, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2402.02526"><font color=blue size=+0.3> CompeteSMoE - Effective training of sparse mixture of experts via competition </font></a>. <i> Under review</i>. 
<br />
Quang Pham, Truong Giang Do, Huy Nguyen, TrungTin Nguyen, Chenghao Liu, Mina Sartipi, Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="Linear_scale_location.pdf"><font color=blue size=+0.3> Beyond EM algorithm on over-specified two-component location-scale Gaussian mixtures</font></a>. <i> Under review</i>. 
<br />
Tongzheng Ren<font color=red size=-0.3><b>*</b></font>, Fuheng Cui<font color=red size=-0.3><b>*</b></font>, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="arxiv_Federated_Self_supervised_Learning_for_Heterogeneous_Clients.pdf"<font color=blue size=+0.3> Federated self-supervised learning for heterogeneous clients</font></a>. <i> Under review</i>.
<br />
Disha Makhija, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Joydeep Ghosh<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2211.13723.pdf"><font color=blue size=+0.3> Improving multi-task learning via seeking task-based flat regions</font></a>. <i> Under review</i>. 
<br />
Hoang Phan, Tung Tran, Ngoc Tran, <b>Nhat Ho</b>, Dinh Phung, Trung Le.</p>
</li>
</ul>
<h2 style="color:steelblue;">Journal Under Revision</h2>
<hr>
<ul>
<li><p><a href="JMLR_V1.pdf"><font color=blue size=+0.3> Multivariate smoothing via the Fourier integral theorem and Fourier kernel </font></a>. <i> Under revision</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="Short_Arxiv_version.pdf"><font color=blue size=+0.3> An exponentially increasing step-size for parameter estimation in statistical models</font></a>. <i> Under revision</i>. 
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Tongzheng Ren<font color=red size=-0.3><b>**</b></font>, Purnamrita Sarkar<font color=red size=-0.3><b>**</b></font>, Sujay Sanghavi<font color=red size=-0.3><b>**</b></font>, Rachel Ward<font color=red size=-0.3><b>**</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.06448.pdf"><font color=blue size=+0.3> Probabilistic best subset selection via gradient-based optimization</font></a>. <i>Under revision</i>. 
<br />
Mingzhang Yin, <b>Nhat Ho</b>, Bowei Yan, Xiaoning Qian, Mingyuan Zhou.</p>
</li>
</ul>
<ul>
<li><p><a href="DPMM_Extension_Work.pdf"><font color=blue size=+0.3> On posterior inference for the number of clusters in Dirichlet process mixture models</font></a>. <i>Under revision</i>. 
<br />
Chiao-Yu Yang, Eric Xia, <b>Nhat Ho</b>, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1912.05153.pdf"><font color=blue size=+0.3> Sampling for Bayesian mixture models: MCMC with polynomial-time mixing</font></a>. <i>Under revision</i>.
<br />
Wenlong Mou, <b>Nhat Ho</b>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan. </p>
</li>
</ul>
<h2 style="color:steelblue;">Journal Publications</h2>
<hr>
<ul>
<li><p> [J.18] <a href="Global_Convergence_EM_MoE.pdf"><font color=blue size=+0.3> Global optimality of the EM algorithm for mixtures of two linear regression </font></a>. <i>IEEE Transactions on Information Theory, 2024</i>.
<br />
Jeongyeol Kwon, Wei Qian, Constantine Caramanis, Yudong Chen, Damek Davis, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [J.17] <a href="https://arxiv.org/pdf/2102.02756.pdf"><font color=blue size=+0.3> On the computational and statistical complexity of over-parameterized matrix sensing </font></a>. <i>Journal of Machine Learning Research (JMLR), 2024</i>.
<br />
Jiacheng Zhuo, Jeongyeol Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p> [J.16] <a href="main_version.pdf"><font color=blue size=+0.3> On integral theorems and their statistical properties </font></a>. <i> Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 2024</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p> [J.15] <a href="Statistical_and_Computational_Complexities_of_BFGS.pdf"><font color=blue size=+0.3> Statistical and computational complexities of BFGS quasi-Newton method for generalized linear models</font></a>. <i> Transactions on Machine Learning Research (TLMR), 2024</i>. 
<br />
Qiujiang Jin, Tongzheng Ren, <b>Nhat Ho</b>, Aryan Mokhtari.</p>
</li>
</ul>
<ul>
<li><p> [J.14] <a href="https://arxiv.org/pdf/2005.11411.pdf"><font color=blue size=+0.3> Instability, computational efficiency, and statistical accuracy </font></a>. <i>Accepted with 
minor revision at Journal of Machine Learning Research (JMLR), 2023</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p>[J.13] <a href="SDE_Arxiv_submission_v2.pdf"><font color=blue size=+0.3> A diffusion process perspective on the posterior contraction rates for parameters</font></a>. <i> SIAM Journal on Mathematics of Data Science (SIMODS), 2023</i>.
<br />
Wenlong Mou, <b>Nhat Ho</b>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p> [J.12] <a href="https://arxiv.org/pdf/1811.02657.pdf"><font color=blue size=+0.3> A Bayesian perspective of convolutional neural networks through a deconvolutional generative model</font></a>. <i>Journal of Machine Learning Research (JMLR), 2023</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Ankit Patel, Anima Anandkumar, Michael I. Jordan, Richard Baraniuk. </p>
</li>
</ul>
<ul>
<li><p> [J.11] <a href="Bayesian_Consistency_Smooth_Final.pdf"><font color=blue size=+0.3> Bayesian consistency with the supremum metric </font></a>. <i> Statistica Sinica, 2022</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p> [J.10] <a href="Mixture_Experts_Arxiv.pdf"><font color=blue size=+0.3> Convergence rates for Gaussian mixtures of experts</font></a>. <i>Journal of Machine Learning Research (JMLR), 2022</i>.
<br />
<b>Nhat Ho</b>, Chiao-Yu Yang, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p> [J.9] <a href="https://arxiv.org/pdf/1906.01437.pdf"><font color=blue size=+0.3> On the efficiency of entropic regularized
algorithms for optimal transport </font></a>. <i> Journal of Machine Learning Research (JMLR), 2022</i>. 
<br />
Tianyi Lin, <b>Nhat Ho</b>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p> [J.8] <a href="https://arxiv.org/pdf/1910.00152.pdf"><font color=blue size=+0.3> On the complexity of approximating multi-marginal optimal transport</font></a>. <i>Journal of Machine Learning Research (JMLR), 2022</i>. 
<br />
Tianyi Lin<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Marco Cuturi, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p> [J.7] <a href="https://arxiv.org/pdf/1901.05078.pdf"><font color=blue size=+0.3> On posterior contraction of parameters and interpretability in Bayesian mixture modeling </font></a>. <i> Bernoulli 27 (4), 2159-2188, 2021</i>.
<br />
Aritra Guha, <b>Nhat Ho</b>, XuanLong Nguyen.</p>
</li>
</ul>
<ul>	
<li><p> [J.6] <a href="https://arxiv.org/pdf/1810.00828.pdf"><font color=blue size=+0.3> Singularity,
misspecification, and the convergence rate of EM</font></a>. <i>Annals of Statistics, 48(6), 3161-3182, 2020</i>. 
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p> [J.5] <a href="https://www.jmlr.org/papers/volume22/19-782/19-782.pdf"><font color=blue size=+0.3> On efficient multilevel clustering via Wasserstein distances</font></a>. <i>Journal of Machine Learning Research (JMLR), 22, 1-43, 2021</i>.
<br />
Viet Huynh<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Nhan Dam, XuanLong Nguyen, Mikhail Yurochkin, Hung Bui, Dinh Phung.</p>
</li>
</ul>
<ul>
<li><p> [J.4] <a href="https://arxiv.org/pdf/1709.08094.pdf"><font color=blue size=+0.3> Robust estimation of mixing measures in finite mixture models</font></a>. <i>Bernoulli, 26(2), 828-857, 2020</i>.
<br />
<b>Nhat Ho</b>, XuanLong Nguyen, Ya'acov Ritov.</p>
</li>
</ul>
<ul>
<li><p> [J.3] <a href="https://arxiv.org/pdf/1609.02655.pdf"><font color=blue size=+0.3> Singularity structures and impacts on parameter estimation behavior in finite mixtures of distributions</font></a>. <i>SIAM Journal on Mathematics of Data Science (SIMODS), 1(4), 730â€“758, 2019</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen.  </p>
</li>
</ul>
<ul>
<li><p> [J.2] <a href="AoS_2016.pdf"><font color=blue size=+0.3>Convergence rates of parameter estimation for some weakly identifiable finite mixtures</font></a>. <i>Annals of Statistics, 44(6), 2726-2755, 2016</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen. </p>
</li>
</ul>
<ul>
<li><p> [J.1] <a href="Ejs_2016.pdf"><font color=blue size=+0.3>On strong identifiability and convergence rates of
parameter estimation in finite mixtures</font></a>. <i>Electronic Journal of Statistics, 10(1), 271-307, 2016</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen.</p>
</li>
</ul>
<h2 style="color:steelblue;">Conference Publications (Machine Learning, Computer Vision, Signal Processing, Time Series Analysis)</h2>
<hr>
<ul>
<li><p>[C.79] <a href="Cosine_MoE_v2.pdf"><font color=blue size=+0.3> 	
Statistical advantages of perturbing cosine router in sparse mixture of experts </font></a>. <i> ICLR, 2025</i>. 
<br />
Huy Nguyen, Pedram Akbarian<font color=red size=-0.3><b>*</b></font>, Trang Pham<font color=red size=-0.3><b>*</b></font>, Trang Nguyen<font color=red size=-0.3><b>*</b></font>, Shujian Zhang <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.78]<a href="https://arxiv.org/abs/2410.02200"><font color=blue size=+0.3> Revisiting prefix-tuning: Statistical benefits of reparameterization among prompts </font></a>. <i> ICLR, 2025</i>. 
<br />
Minh Le<font color=red size=-0.3><b>*</b></font>, Chau Nguyen<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Quyen Tran, Trung Le, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.77]<a href="https://arxiv.org/abs/2410.12258"><font color=blue size=+0.3> X-Drive: Cross-modality consistent multi-sensor data synthesis for driving scenarios </font></a>. <i> ICLR, 2025</i>. 
<br />
Yichen Xie, Chenfeng Xu, Chensheng Peng, Shuqi Zhao, <b>Nhat Ho</b>, Alexander T. Pham, Mingyu Ding, Wei Zhan, Masayoshi Tomizuka.</p>
</li>
</ul>
<ul>
<li><p>[C.76] <a href="https://arxiv.org/abs/2405.07482"><font color=blue size=+0.3> Marginal fairness sliced Wasserstein barycenter </font></a>. <i> ICLR, 2025</i>. 
<br />
Khai Nguyen, Hai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.75] <a href="https://arxiv.org/abs/2410.12258"><font color=blue size=+0.3> Understanding expert structures on minimax parameter estimation in contaminated mixture of experts </font></a>. <i> AISTATS, 2025</i>. 
<br />
Fanqi Yan<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Dung Le<font color=red size=-0.3><b>*</b></font>, Pedram Akbarian, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.74] <a href="Backdoor_Continual_Learning_v2.pdf"><font color=blue size=+0.3> Backdoor attack in prompt-based continual learning </font></a>. <i> AAAI, 2025</i>. 
<br />
Trang Nguyen, Anh Tran, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.73] <a href="Sigmoid_MoE.pdf"><font color=blue size=+0.3> Sigmoid gating is more sample efficient than softmax gating in mixture of experts </font></a>. <i> Advances in NeurIPS, 2024</i>. 
<br />
Huy Nguyen, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Alessandro Rinaldo<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[C.72] <a href="https://arxiv.org/abs/2401.15771"><font color=blue size=+0.3> Bayesian nonparametrics meets data-driven distributionally robust optimization </font></a>. <i> Advances in NeurIPS, 2024</i>. 
<br />
Nicola Bariletto, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.71] <a href="https://arxiv.org/pdf/2402.03226.pdf"><font color=blue size=+0.3> FuseMoE: Mixture-of-experts Transformers for fleximodal fusion </font></a>. <i> Advances in NeurIPS, 2024</i>. 
<br />
Xing Han, Huy Nguyen, Carl William Harris,  <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Suchi Saria<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[C.70] <a href="MoE_Continual_Learning.pdf"><font color=blue size=+0.3> 	
Mixture of experts meets prompt-based continual learning </font></a>. <i> Advances in NeurIPS, 2024</i>. 
<br />
Minh Le, An Nguyen<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Trang Nguyen<font color=red size=-0.3><b>*</b></font>, Trang Pham<font color=red size=-0.3><b>*</b></font>, Linh Van Ngo, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.69] <a href="https://arxiv.org/abs/2404.15378"><font color=blue size=+0.3> Hierarchical hybrid sliced Wasserstein: A scalable metric for heterogeneous joint distributions </font></a>. <i> Advances in NeurIPS, 2024</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.68] <a href="arxiv_Federated_Self_supervised_Learning_for_Heterogeneous_Clients.pdf"<font color=blue size=+0.3> A Bayesian approach for personalized federated learning in heterogeneous settings</font></a>. <i> Advances in NeurIPS, 2024</i>.
<br />
Disha Makhija, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Joydeep Ghosh<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[C.67] <a href="https://arxiv.org/pdf/2401.13875.pdf"><font color=blue size=+0.3> Is temperature sample efficient for softmax Gaussian mixture of experts? </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Huy Nguyen, Pedram Akbarian, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.66] <a href="https://arxiv.org/abs/2402.02952"><font color=blue size=+0.3> On least square estimation in softmax gating mixture of experts </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Huy Nguyen, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Alessandro Rinaldo<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[C.65] <a href="https://arxiv.org/abs/2401.15889"><font color=blue size=+0.3> Sliced Wasserstein with random-path projecting directions </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Khai Nguyen, Shujian Zhang, Tam Le, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.64] <a href="https://arxiv.org/abs/2401.02058"><font color=blue size=+0.3> Neural collapse for cross-entropy class-imbalanced learning with unconstrained ReLU feature model </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Hien Dang, Tho Tran, Tan Nguyen<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[C.63] <a href="https://arxiv.org/abs/2402.01975"><font color=blue size=+0.3> Structure-aware E(3)-invariant molecular conformer Aggregation Networks </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Duy Minh Ho Nguyen, Nina Lukashina, Tai Nguyen, An Thai Le, TrungTin Nguyen, <b>Nhat Ho</b>, Jan Peters, Daniel Sonntag, Viktor Zaverkin, Mathias Niepert.</p>
</li>
</ul>
<ul>
<li><p>[C.62] <a href="https://arxiv.org/pdf/2309.13850.pdf"><font color=blue size=+0.3> A general theory for softmax gating multinomial logistic mixture of experts </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Huy Nguyen, Pedram Akbarian, Tin Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.61] <a href="Normalized_GD.pdf"<font color=blue size=+0.3> Improving computational complexity in statistical models with second-order information </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Tongzheng Ren, Pedram Akbarian, Jiacheng Zhuo, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.60] <a href="https://openreview.net/forum?id=Wd47f7HEXg"><font color=blue size=+0.3> Quasi-Monte Carlo for 3D sliced Wasserstein </font></a>. <i> ICLR, 2024 (Spotlight)</i>. 
<br />
Khai Nguyen, Nicola Bariletto, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.59] <a href="https://openreview.net/forum?id=jvtmdK69KQ"><font color=blue size=+0.3> Statistical perspective of top-K sparse softmax gating mixture of experts </font></a>. <i> ICLR, 2024</i>. 
<br />
Huy Nguyen, Pedram Akbarian, Fanqi Yan, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.58] <a href="https://openreview.net/forum?id=4zZFGliCl9"><font color=blue size=+0.3> Beyond vanilla variational autoencoders: Detecting posterior collapse in conditional and hierarchical variational autoencoders </font></a>. <i> ICLR, 2024</i>. 
<br />
Hien Dang, Tho Tran, Tan Nguyen<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[C.57] <a href="https://openreview.net/forum?id=StYc4hQAEi"><font color=blue size=+0.3> Sliced Wasserstein estimation with control variates </font></a>. <i> ICLR, 2024</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.56] <a href="https://openreview.net/forum?id=gxhRR8vUQb"><font color=blue size=+0.3> Diffeomorphic mesh deformation via efficient optimal transport for cortical surface reconstruction</font></a>. <i> ICLR, 2024</i>. 
<br />
Tung Le, Khai Nguyen, Shanlin Sun, Kun Han, <b>Nhat Ho</b>, Xiaohui Xie.</p>
</li>
</ul>
<ul>
<li><p>[C.55] <a href="https://openreview.net/forum?id=l60EM8md3t"><font color=blue size=+0.3> Revisiting deep audio-text retrieval through the lens of transportation</font></a>. <i> ICLR, 2024</i>. 
<br />
Manh Luong, Khai Nguyen, <b>Nhat Ho</b>, Reza Haf, Dinh Phung, Lizhen Qu.</p>
</li>
</ul>
<ul>
<li><p>[C.54] <a href="https://arxiv.org/pdf/2305.07572.pdf"><font color=blue size=+0.3> Towards convergence rates for parameter estimation
in Gaussian-gated mixture of experts </font></a>. <i> AISTATS, 2024</i>. 
<br />
Huy Nguyen, Tin Nguyen, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.53] <a href="https://arxiv.org/abs/2402.05220"><font color=blue size=+0.3> On parameter estimation in deviated Gaussian mixture of experts</font></a>. <i> AISTATS, 2024</i>. 
<br />
Huy Nguyen, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.52] <a href="OT_Shape_Correspondence.pdf"><font color=blue size=+0.3> Integrating efficient optimal transport and functional maps For unsupervised shape correspondence learning</font></a>. <i> Conference on Computer Vision and Pattern Recognition (CVPR), 2024</i>. 
<br />
Thanh Tung Le, Khai Nguyen, shanlin sun, <b>Nhat Ho</b>, Xiaohui Xie.</p>
</li>
</ul>
<ul>
<li><p>[C.51] <a href="https://arxiv.org/pdf/2210.10268.pdf"><font color=blue size=+0.3> Fast approximation of the generalized sliced-Wasserstein distance</font></a>. <i> IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2024</i>. 
<br />
Dung Le<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Khai Nguyen<font color=red size=-0.3><b>*</b></font>, Trang Nguyen<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.50] <a href="https://arxiv.org/pdf/2305.03288.pdf"><font color=blue size=+0.3> Demystifying softmax gating function in Gaussian mixture of
experts </font></a>. <i> Advances in NeurIPS, 2023 (Spotlight)</i>. 
<br />
Huy Nguyen, Tin Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.49] <a href="https://arxiv.org/pdf/2304.13586.pdf"><font color=blue size=+0.3> Energy-based sliced Wasserstein distance </font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.48] <a href="https://arxiv.org/pdf/2301.11808.pdf"><font color=blue size=+0.3> Minimax optimal rate for parameter estimation in multivariate deviated models</font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Dat Do<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.47] <a href="https://arxiv.org/pdf/2301.03749.pdf"><font color=blue size=+0.3> Markovian sliced Wasserstein distances: Beyond
independent projections </font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Khai Nguyen, Tongzheng Ren, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.46] <a href="https://arxiv.org/pdf/2210.05794.pdf"><font color=blue size=+0.3> Designing robust transformers using robust kernel density estimation</font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.45] <a href="https://arxiv.org/pdf/2306.11925.pdf"><font color=blue size=+0.3> LVM-Med: Learning large-Scale self-Supervised vision models for medical imaging via second-order graph matching</font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Duy Nguyen, Hoang Nguyen, Nghiem Diep, Tan Pham, Tri Cao, Binh T.  Nguyen, Paul Swoboda, <b>Nhat Ho</b>, Shadi Albarqouni, Pengtao Xie, Mathias Niepert, Daniel Sonntag.</p>
</li>
</ul>
<ul>
<li><p>[C.44] <a href="https://arxiv.org/pdf/2301.11496.pdf"><font color=blue size=+0.3> On excess mass behavior in Gaussian mixture models with Orlicz Wasserstein distances </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Aritra Guha, <b>Nhat Ho</b>, Long Nguyen.</p>
</li>
</ul>
<ul>
<li><p>[C.43] <a href="https://arxiv.org/pdf/2301.00437.pdf"><font color=blue size=+0.3> Neural collapse in deep linear network: from balanced to imbalanced data </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Hien Dang<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Tho Tran, Stanley Osher, Hung Tran, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[C.42] <a href="https://arxiv.org/pdf/2301.04791.pdf"><font color=blue size=+0.3> Self-attention amortized distributional projection optimization for sliced
Wasserstein point-clouds reconstruction </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Khai Nguyen<font color=red size=-0.3><b>*</b></font>, Dang Nguyen<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.41] <a href="https://arxiv.org/pdf/2211.15779.pdf"><font color=blue size=+0.3> Revisiting over-smoothing and over-squashing using Ollivier's Ricci curvature </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Khang Nguyen, Tan Minh Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[C.40] <a href="Hierarchical_SW.pdf"><font color=blue size=+0.3> Hierarchical sliced Wasserstein distance</font></a>. <i> ICLR, 2023</i>. 
<br />
Khai Nguyen, Tongzheng Ren, Huy Nguyen, Litu Rout, Tan Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.39] <a href="ICLR_Primal_Dual.pdf"><font color=blue size=+0.3> A primal-dual framework for transformers and neural networks</font></a>. <i> ICLR, 2023 (Spotlight)</i>. 
<br />
Tan Minh Nguyen, Tam Minh Nguyen, <b>Nhat Ho</b>, Andrea L. Bertozzi, Richard Baraniuk, Stanley Osher.</p>
</li>
</ul>
<ul>
<li><p>[C.38] <a href="GLOT_arxiv.pdf" ><font color=blue size=+0.3> Global-local regularization via distributional robustness</font></a>. <i> AISTATS, 2023</i>.
<br />
Hoang Phan, Trung Le, Trung Phung, Tuan Anh Bui, <b>Nhat Ho</b>, Dinh Phung.</p>
</li>
</ul>
<ul>
<li><p>[C.37]<a href="https://arxiv.org/pdf/2110.15538.pdf"><font color=blue size=+0.3> Model fusion of heterogeneous neural networks via cross-layer alignment </font></a>. <i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2023 (Top 3%)</i>. 
<br />
Dang Nguyen, Trang Nguyen, Khai Nguyen, Dinh Phung, Hung Bui, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.36] <a href="FiAK_Transformer.pdf"><font color=blue size=+0.3> A probabilistic framework for pruning transformers via a finite admixture of keys </font></a>. <i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2023 (Top 3%)</i>.
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Long Bui, Hai Do, Dung Le, Hung Tran-The, Khuong Nguyen, <b>Nhat Ho</b>, Stanley Osher, Richard Baraniuk.</p>
</li>
</ul>
<ul>
<li><p>[C.35] <a href="https://arxiv.org/pdf/2212.01893.pdf"> <font color=blue size=+0.3>  Joint self-supervised image-volume representation learning with intra-inter contrastive clustering </font></a>. <i> AAAI, 2023</i>. 
<br />
Duy Nguyen, Nguyen Hoang, Truong Mai, Cao Tri, Binh Nguyen, <b>Nhat Ho</b>, Paul Swoboda, Shadi Albarqouni, Pengtao Xie, Daniel Sonntag.</p>
</li>
</ul>
<ul>
<li><p>[C.34]<a href="HTS_Cluster_arxiv.pdf"<font color=blue size=+0.3> Efficient forecasting of large scale hierarchical time series via multilevel clustering </font></a>. <i> International conference on Time Series and Forecasting (ITISE), 2023</i>. 
<br />
Xing Han, Tongzheng Ren, Jing Hu, Joydeep Ghosh, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.33] <a href="https://arxiv.org/pdf/2204.01188.pdf"><font color=blue size=+0.3> Revisiting projected Wasserstein metric on images: from vectorization to convolution</font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.32] <a href="https://arxiv.org/pdf/2206.00206.pdf"><font color=blue size=+0.3> FourierFormer: Transformer meets generalized Fourier integral attentions </font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Minh Pham<font color=red size=-0.3><b>*</b></font>, Tam Nguyen, Khai Nguyen, Stanley Osher, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.31] <a href="https://arxiv.org/pdf/2203.13417.pdf"><font color=blue size=+0.3> Amortized projection optimization for sliced Wasserstein generative models</font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[C.30] <a href="https://openreview.net/pdf/872ba8e8fd9e355923f481f12b29ab2748bfa192.pdf"><font color=blue size=+0.3>  Improving Transformer with an admixture of attention heads </font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Hai Do, Khai Nguyen, Vishwanath Saragadam, Minh Pham, Khuong Nguyen,Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[C.29] <a href="https://arxiv.org/pdf/2202.02651.pdf"<font color=blue size=+0.3> Beyond black box densities: Parameter learning for the deviated components</font></a>. <i>Advances in NeurIPS, 2022</i>.
<br />
Dat Do<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Long Nguyen.</p>
</li>
</ul>
<ul>
<li><p>[C.28] <a href="https://arxiv.org/pdf/2206.01934.pdf"><font color=blue size=+0.3> Stochastic multiple target sampling gradient descent
</font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Hoang Phan, Ngoc Tran, Trung Le, Toan Tran, <b>Nhat Ho</b>, Dinh Phung.</p>
</li>
</ul>
<ul>
<li><p> [C.27] <a href="https://arxiv.org/pdf/2202.08786.pdf"<font color=blue size=+0.3> Refined convergence rates for maximum likelihood estimation under finite mixture models</font></a>. <i> Proceedings of the ICML, 2022 (Long Presentation)</i>. 
<br />
Tudor Manole, <b>Nhat Ho</b>.
</li>
</ul>
<ul>
<li><p> [C.26] <a href="https://arxiv.org/pdf/2110.08678.pdf"<font color=blue size=+0.3> Improving Transformers with probabilistic attention keys </font></a>. <i>Proceedings of the ICML, 2022</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Dung Le, Khuong Nguyen, Anh Tran, Richard Baraniuk, Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p> [C.25] <a href="https://arxiv.org/pdf/2108.10961.pdf"><font color=blue size=+0.3>  Entropic Gromov-Wasserstein between Gaussian distributions</font></a>. <i>Proceedings of the ICML, 2022</i>. 
<br />
Khang Le<font color=red size=-0.3><b>*</b></font>, Dung Le<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Dat Do, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [C.24] <a href="https://arxiv.org/pdf/2108.09645.pdf"><font color=blue size=+0.3>  Improving minibatch optimal transport via partial transportation</font></a>. <i>Proceedings of the ICML, 2022</i>. 
<br />
Khai Nguyen, Dang Nguyen, Anh Le, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [C.23] <a href="https://arxiv.org/pdf/2102.05912.pdf"><font color=blue size=+0.3> On transportation of mini-batches: A hierarchical approach </font></a>. <i>Proceedings of the ICML, 2022</i>.
<br />
Khai Nguyen, Dang Nguyen, Quoc Nguyen, Tung Pham, Dinh Phung, Hung Bui, Trung Le, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [C.22] <a href="https://arxiv.org/pdf/2202.07757.pdf"<font color=blue size=+0.3> Architecture agnostic federated learning for neural networks</font></a>. <i> Proceedings of the ICML, 2022</i>.
<br />
Disha Makhija, Xing Han, <b>Nhat Ho</b>, Joydeep Ghosh.</p>
</li>
</ul>
<ul>
<li><p> [C.21] <a href="Polyak_Step_Size_V3.pdf"><font color=blue size=+0.3> Towards statistical and computational complexities of Polyak step size gradient descent </font></a>. <i>AISTATS, 2022</i>. 
<br />
Tongzheng Ren<font color=red size=-0.3><b>*</b></font>, Fuheng Cui<font color=red size=-0.3><b>*</b></font>, Alexia Atsidakou<font color=red size=-0.3><b>*</b></font>, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [C.20] <a href="https://arxiv.org/pdf/2108.07992.pdf"><font color=blue size=+0.3>  On multimarginal partial optimal transport: Equivalent forms and computational complexity</font></a>. <i>AISTATS, 2022</i>. 
<br />
Khang Le<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [C.19] <a href="Trend_Filtering_AISTATS.pdf"><font color=blue size=+0.3> On structured filtering-clustering: Global error bound and optimal first-order algorithms</font></a>. <i>AISTATS, 2022</i>. 
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Tianyi Lin<font color=red size=-0.3><b>*</b></font>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p> [C.18] <a href="Weak_Separation_AISTATS.pdf"><font color=blue size=+0.3> Weak separation in mixture models and implications for principal stratification</font></a>. <i>AISTATS, 2022</i>.
<br /> 
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Avi Feller<font color=red size=-0.3><b>**</b></font>, Evan Greif<font color=red size=-0.3><b>**</b></font>, Luke W. Miratrix<font color=red size=-0.3><b>**</b></font>, Natesh S.
Pillai<font color=red size=-0.3><b>**</b></font>. </p>
</li>
</ul>
<ul>
<li><p> [C.17] <a href="https://arxiv.org/pdf/2102.06857.pdf"><font color=blue size=+0.3> On robust optimal transport: Computational complexity and barycenter computation </font></a>. <i>Advances in NeurIPS, 2021</i>.
<br />
Khang Le<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Quang Nguyen, Tung Pham, Hung Bui<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p> [C.16] <a href="https://arxiv.org/pdf/2102.07927.pdf"><font color=blue size=+0.3> Structured Dropout variational inference for Bayesian neural networks </font></a>. <i>Advances in NeurIPS, 2021</i>.
<br />
Son Nguyen<font color=red size=-0.3><b>*</b></font>, Duong Nguyen<font color=red size=-0.3><b>*</b></font>, Khai Nguyen, Khoat Than, Hung Bui<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p> [C.15] <a href="LAMDA_ICML.pdf"><font color=blue size=+0.3> LAMDA: Label matching deep domain adaptation</font></a>. <i>Proceedings of the ICML, 2021</i>.
<br />
Trung Le, Tuan Nguyen, <b>Nhat Ho</b>, Hung Bui, Dinh Phung.  </p>
</li>
</ul>
<ul>
<li><p> [C.14] <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Nguyen_Point-Set_Distances_for_Learning_Representations_of_3D_Point_Clouds_ICCV_2021_paper.pdf"><font color=blue size=+0.3> Point-set distances for learning representations of 3D point clouds</font></a>. <i>International Conference on Computer Vision (ICCV), 2021</i>.
<br />
Trung Nguyen, Hieu Pham, Tam Le, Tung Pham, <b>Nhat Ho</b>, Son Hua.  </p>
</li>
</ul>
<ul>
<li><p> [C.13] <a href="https://arxiv.org/pdf/2006.02601.pdf"><font color=blue size=+0.3> On the minimax optimality of the EM algorithm for learning two-component mixed linear regression</font></a>. <i> AISTATS, 2021</i>. 
<br />
Jeong Y. Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p> [C.12] <a href="Tree_Gromov_AISTATS.pdf"><font color=blue size=+0.3> Flow-based alignment approaches
for probability measures in different spaces</font></a>. <i>AISTATS, 2021</i>. 
<br />
Tam Le<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Makoto Yamada.</p>
</li>
</ul>
<ul>
<li><p> [C.11] <a href="https://arxiv.org/pdf/2002.07367.pdf"><font color=blue size=+0.3> Distributional sliced-Wasserstein and applications to deep generative modeling</font></a>. <i>ICLR, 2021 (Spotlight)</i>.
<br />
Khai Nguyen, <b>Nhat Ho</b>, Tung Pham, Hung Bui.  </p>
</li>
</ul>
<ul>
<li><p> [C.10] <a href="https://arxiv.org/pdf/2010.01787.pdf"><font color=blue size=+0.3> Improving relational regularized autoencoders with spherical sliced fused Gromov Wasserstein</font></a>. <i>ICLR, 2021</i>.
<br />
Khai Nguyen, Son Nguyen, <b>Nhat Ho</b>, Tung Pham, Hung Bui.  </p>
</li>
</ul>
<ul>
<li><p> [C.9] <a href="https://arxiv.org/pdf/2006.07458.pdf"><font color=blue size=+0.3> Projection robust Wasserstein distance and Riemannian optimization </font></a>. <i> Advances in NeurIPS, 2020 (Spotlight)</i>. 
<br />
Tianyi Lin<font color=red size=-0.3><b>*</b></font>, Chenyou Fan<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>, Marco Cuturi, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p> [C.8] <a href="https://arxiv.org/pdf/2002.04783.pdf"><font color=blue size=+0.3> Fixed-support Wasserstein barycenters:
computational hardness and fast algorithm </font></a>. <i> Advances in NeurIPS, 2020</i>. 
<br />
Tianyi Lin, <b>Nhat Ho</b>, Xi Chen, Marco Cuturi, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p> [C.7] <a href="https://arxiv.org/pdf/2002.03293.pdf"><font color=blue size=+0.3> On unbalanced optimal transport: an analysis of Sinkhorn algorithm</font></a>. <i>Proceedings of the ICML, 2020</i>. 
<br />
Khiem Pham<font color=red size=-0.3><b>*</b></font>, Khang Le<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>, Tung Pham, Hung Bui.</p>
</li>
</ul>
<ul>
<li><p> [C.6] <a href="https://arxiv.org/pdf/1902.00194.pdf"><font color=blue size=+0.3> Sharp analysis of Expectation-Maximization for weakly identifiable models</font></a>. <i>AISTATS, 2020</i>.
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p> [C.5] <a href="https://arxiv.org/pdf/1905.09952.pdf"><font color=blue size=+0.3> Fast algorithms for computational optimal transport and Wasserstein barycenter</font></a>. <i>AISTATS, 2020</i>. 
<br />
Wenshuo Guo, <b>Nhat Ho</b>, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p>[C.4] <a href="https://arxiv.org/pdf/1901.06482.pdf"><font color=blue size=+0.3> On efficient optimal transport: an analysis of greedy and accelerated mirror descent algorithms</font></a>. <i>Proceedings of the ICML, 2019</i>.
<br />
Tianyi Lin<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p> [C.3] <a href="https://arxiv.org/pdf/1810.11911.pdf"><font color=blue size=+0.3> Probabilistic multilevel clustering via composite transportation distance</font></a>. <i>AISTATS, 2019</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Viet Huynh<font color=red size=-0.3><b>*</b></font>, Dinh Phung, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p> [C.2] <a href="EM_misspecified_unified.pdf"><font color=blue size=+0.3> Theoretical guarantees for EM under misspecified Gaussian mixture models </font></a>. <i>Advances in NeurIPS, 2018</i>.
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p>[C.1] <a href="https://arxiv.org/pdf/1706.03883.pdf"><font color=blue size=+0.3> Multilevel clustering via Wasserstein means</font></a>. <i>Proceedings of the ICML, 2017</i>.
<br />
<b>Nhat Ho</b>, XuanLong Nguyen, Mikhail Yurochkin, Hung Hai Bui, Viet Huynh, and Dinh Phung.</p>
</li>
</ul>
<h2 style="color:steelblue;"> Phd Thesis/ Manuscripts</h2>
<ul>
<li><p><a href="https://deepblue.lib.umich.edu/bitstream/handle/2027.42/140958/minhnhat_1.pdf?sequence=1&isAllowed=y"><font color=blue size=+0.3> Parameter Estimation and Multilevel Clustering with Mixture and Hierarchical Models </font></a>. <i>Phd Thesis, University of Michigan, Ann Arbor</i>. Advisors: Professor <a href="http://dept.stat.lsa.umich.edu/~xuanlong/">Long Nguyen</a> and Professor <a href="http://www-personal.umich.edu/~yritov/jr.html">Ya'acov Ritov</a>.
</li>
</ul>
<hr>
<div id="footer">
<div id="footer-text">
Page generated 2020-08-10 20:31:45 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
