<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><br /></div>
<div class="menu-category">Nhat Ho</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publications_type.html">Publications&nbsp;(by&nbsp;type)</a></div>
<div class="menu-item"><a href="publications_topics.html">Publications&nbsp;(by&nbsp;topic)</a></div>
<div class="menu-item"><a href="publications_year.html">Publications&nbsp;(by&nbsp;year)</a></div>
<div class="menu-item"><a href="collaborators.html">Students/&nbsp;Collaborators</a></div>
<div class="menu-item"><a href="workshops.html">Workshops/&nbsp;Talks/&nbsp;Lectures</a></div>
</td>
<td id="layout-content">
<p><br /></p>
<h1>Preprints/ Publications (by year)</h1>
<p>(<font color=red size=-0.3><b>* = equal contribution </b></font>)
<br /> 
(<font color=red size=-0.3><b>** = alphabetical order </b></font>)
<br />
(<font color=red size=-0.3><b><span>&dagger;</span> = co-last author </b></font>)
<br /></p>
<h2 style="color:steelblue;">Journal Submissions</h2>
<hr>
<ul>
<li><p><a href="JMLR_V1.pdf"><font color=blue size=+0.3> Multivariate smoothing via the Fourier integral theorem and Fourier kernel </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="Statistical_Fourier_Theorem.pdf"><font color=blue size=+0.3> Generative models from the multivariate Fourier integral theorem </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="main_version.pdf"><font color=blue size=+0.3> On integral theorems: Monte Carlo estimators and optimal functions </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.06448.pdf"><font color=blue size=+0.3> Probabilistic best subset selection via gradient-based optimization</font></a>. <i>Under review</i>. 
<br />
Mingzhang Yin, <b>Nhat Ho</b>, Bowei Yan, Xiaoning Qian, Mingyuan Zhou.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Global optimality of the EM algorithm for mixtures of two linear regression </font></a>. <i>Under review</i>.
<br />
Jeongyeol Kwon, Wei Qian, Constantine Caramanis, Yudong Chen, Damek Davis, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Short_Arxiv_version.pdf"><font color=blue size=+0.3> An exponentially increasing step-size for parameter estimation in statistical models</font></a>. <i> Under review</i>. 
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Tongzheng Ren<font color=red size=-0.3><b>**</b></font>, Purnamrita Sarkar<font color=red size=-0.3><b>**</b></font>, Sujay Sanghavi<font color=red size=-0.3><b>**</b></font>, Rachel Ward<font color=red size=-0.3><b>**</b></font>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> A general theory for softmax gating Gaussian mixture of
experts </font>. <i> To be submitted</i>.
<br />
Huy Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Energy meets sliced Wasserstein distance: From linear to nonlinear projections </font>. <i> To be submitted</i>.
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Mixture model meets Transformer </font>. <i>To be submitted</i>.
<br />
Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Tam Nguyen<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Improving Transformer via general integral theorems </font>. <i>To be submitted</i>.
<br />
Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Minh Pham<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Revisiting sliced optimal transport via geometric methods</font>. <i>To be submitted</i>.
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3>  Joint self-supervised image-volume representation learning with intra-inter contrastive clustering </font>. <i> To be submitted</i>. 
<br />
Duy Nguyen, Nguyen Hoang, Truong Mai, Cao Tri, Binh Nguyen, <b>Nhat Ho</b>, Paul Swoboda, Shadi Albarqouni, Pengtao Xie, Daniel Sonntag.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.00704.pdf"><font color=blue size=+0.3> Uniform convergence rates for maximum likelihood estimation under two-component Gaussian mixture models</font></a>. <i>Arxiv Preprint</i>. 
<br />
Tudor Manole, <b>Nhat Ho</b>.
<br /> <br />
<i>Journal extension</i>:  <font color=blue size=+0.3> Minimax optimal convergence rates for location-scale Gaussian mixture models</font></a>. <i>To be submitted</i>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> On mean-field variational inference: Non-asymptotic approximation, Bayesian inference, and parameter estimation</font>. <i>To be submitted</i>.
<br />
Wenlong Mou, <b>Nhat Ho</b>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="BONuS__Arxiv_.pdf"><font color=blue size=+0.3> BONuS: Multiple multivariate testing with a data-adaptive test statistic </font></a>. <i>Arxiv Preprint</i>.
<br />
Chiao-Yu Yang, Lihua Lei, <b>Nhat Ho</b>, Will Fithian.</p>
</li>
</ul>
<h2 style="color:steelblue;">Conference Submissions</h2>
<hr>
<ul>
<li><p><a href="https://arxiv.org/pdf/2309.13850.pdf"><font color=blue size=+0.3> A general theory for softmax gating multinomial logistic mixture of experts </font></a>. <i> Under review</i>. 
<br />
Huy Nguyen, Pedram Akbarian, Tin Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2305.07572.pdf"><font color=blue size=+0.3> Towards convergence rates for parameter estimation
in Gaussian-gated mixture of experts </font></a>. <i> Under review</i>. 
<br />
Huy Nguyen, Tin Nguyen, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Softmax_MoE.pdf"><font color=blue size=+0.3> CompeteSMOE: Effective training of sparse mixture of experts via competition </font></a>. <i> Under review</i>. 
<br />
Quang Pham, Huy Nguyen, Tin Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> On parameter estimation under contaminated models with covariates</font></a>. <i> Under review</i>. 
<br />
Huy Nguyen, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2305.17555.pdf"><font color=blue size=+0.3> Integrating efficient optimal transport and functional maps for unsupervised shape correspondence learning</font></a>. <i> Under review</i>. 
<br />
Tung Le, Khai Nguyen, Shanlin Sun, <b>Nhat Ho</b>, Xiaohui Xie.</p>
</li>
</ul>
<ul>
<li><p><a href="Linear_scale_location.pdf"><font color=blue size=+0.3> Beyond EM algorithm on over-specified two-component location-scale Gaussian mixtures</font></a>. <i> Under review</i>. 
<br />
Tongzheng Ren<font color=red size=-0.3><b>*</b></font>, Fuheng Cui<font color=red size=-0.3><b>*</b></font>, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Normalized_GD.pdf"<font color=blue size=+0.3> Improving computational complexity in statistical models with second-order information </font></a>. <i> Under review</i>. 
<br />
Tongzheng Ren, Jiacheng Zhuo, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="arxiv_Federated_Self_supervised_Learning_for_Heterogeneous_Clients.pdf"<font color=blue size=+0.3> Privacy preserving Bayesian federated learning in heterogeneous settings</font></a>. <i> Under review</i>.
<br />
Disha Makhija, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Joydeep Ghosh<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="arxiv_Federated_Self_supervised_Learning_for_Heterogeneous_Clients.pdf"<font color=blue size=+0.3> Federated self-supervised learning for heterogeneous clients</font></a>. <i> Under review</i>.
<br />
Disha Makhija, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Joydeep Ghosh<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="Statistical_and_Computational_Complexities_of_BFGS.pdf"><font color=blue size=+0.3> Statistical and computational complexities of BFGS quasi-Newton method for generalized linear models</font></a>. <i> Under review</i>. 
<br />
Qiujiang Jin, Tongzheng Ren, <b>Nhat Ho</b>, Aryan Mokhtari.</p>
</li>
</ul>
<ul>
<li><p><a href="Label_Shift_OT_Theory__Arxiv.pdf"><font color=blue size=+0.3> On label shift in domain adaptation via Wasserstein distance </font></a>. <i>Under review</i>. 
<br />
Trung Le, Dat Do, Tuan Nguyen, Huy Nguyen, Hung Bui, <b>Nhat Ho</b>, Dinh Phung.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2209.15092.pdf"><font color=blue size=+0.3> Improving generative flow networks with path regularization</font></a>. <i> Under review</i>. 
<br />
Anh Hoang, Duy Dinh, Tan Nguyen, Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> Efficient Transformer with compressive sensing </font></a>. <i> Under review</i>. 
<br />
Tan Minh Nguyen, Tho Tran Huu, Tam Minh Nguyen, Minh Pham, Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2211.13723.pdf"><font color=blue size=+0.3> Improving multi-task learning via seeking task-based flat regions</font></a>. <i> Under review</i>. 
<br />
Hoang Phan, Tung Tran, Ngoc Tran, <b>Nhat Ho</b>, Dinh Phung, Trung Le.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1910.04483.pdf"><font color=blue size=+0.3> On scalable variant of Wasserstein barycenter </font></a>. <i>Under review</i>. 
<br />
Tam Le<font color=red size=-0.3><b>*</b></font>, Viet Huynh<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Dinh Phung, Makoto Yamada.</p>
</li>
</ul>
<h2 style="color:steelblue;">Journal Under Revision</h2>
<hr>
<ul>
<li><p><a href="https://arxiv.org/pdf/2102.02756.pdf"><font color=blue size=+0.3> On the computational and statistical complexity of over-parameterized matrix sensing </font></a>. <i>Under revision</i>.
<br />
Jiacheng Zhuo, Jeongyeol Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p><a href="DPMM_Extension_Work.pdf"><font color=blue size=+0.3> On posterior inference for the number of clusters in Dirichlet process mixture models</font></a>. <i>Under revision</i>. 
<br />
Chiao-Yu Yang, Eric Xia, <b>Nhat Ho</b>, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1912.05153.pdf"><font color=blue size=+0.3> Sampling for Bayesian mixture models: MCMC with polynomial-time mixing</font></a>. <i>Under revision</i>.
<br />
Wenlong Mou, <b>Nhat Ho</b>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan. </p>
</li>
</ul>
<h2 style="color:steelblue;">2024</h2>
<hr>
<ul>
<li><p>[73] <a href="https://openreview.net/forum?id=Wd47f7HEXg"><font color=blue size=+0.3> Quasi-Monte Carlo for 3D sliced Wasserstein </font></a>. <i> ICLR, 2024 (Spotlight)</i>. 
<br />
Khai Nguyen, Nicola Bariletto, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[72] <a href="https://openreview.net/forum?id=jvtmdK69KQ"><font color=blue size=+0.3> Statistical perspective of top-K sparse softmax gating mixture of experts </font></a>. <i> ICLR, 2024</i>. 
<br />
Huy Nguyen, Pedram Akbarian, Fanqi Yan, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[71] <a href="https://openreview.net/forum?id=4zZFGliCl9"><font color=blue size=+0.3> Beyond vanilla variational autoencoders: Detecting posterior collapse in conditional and hierarchical variational autoencoders </font></a>. <i> ICLR, 2024</i>. 
<br />
Hien Dang, Tho Tran, Tan Nguyen<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[70] <a href="https://openreview.net/forum?id=StYc4hQAEi"><font color=blue size=+0.3> Sliced Wasserstein estimation with control variates </font></a>. <i> ICLR, 2024</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[69] <a href="https://openreview.net/forum?id=gxhRR8vUQb"><font color=blue size=+0.3> Diffeomorphic mesh deformation via efficient optimal transport for cortical surface reconstruction</font></a>. <i> ICLR, 2024</i>. 
<br />
Tung Le, Khai Nguyen, Shanlin Sun, Kun Han, <b>Nhat Ho</b>, Xiaohui Xie.</p>
</li>
</ul>
<ul>
<li><p>[68] <a href="https://openreview.net/forum?id=l60EM8md3t"><font color=blue size=+0.3> Revisiting deep audio-text retrieval through the lens of transportation</font></a>. <i> ICLR, 2024</i>. 
<br />
Manh Luong, Khai Nguyen, <b>Nhat Ho</b>, Reza Haf, Dinh Phung, Lizhen Qu.</p>
</li>
</ul>
<ul>
<li><p>[67] <a href="https://arxiv.org/pdf/2305.07572.pdf"><font color=blue size=+0.3> Towards convergence rates for parameter estimation
in Gaussian-gated mixture of experts </font></a>. <i> AISTATS, 2024</i>. 
<br />
Huy Nguyen, Tin Nguyen, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[66] <font color=blue size=+0.3> On parameter estimation in deviated Gaussian mixture of experts</font></a>. <i> AISTATS, 2024</i>. 
<br />
Huy Nguyen, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[65] <a href="https://arxiv.org/pdf/2210.10268.pdf"><font color=blue size=+0.3> Fast approximation of the generalized sliced-Wasserstein distance</font></a>. <i> IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2024</i>. 
<br />
Dung Le<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Khai Nguyen<font color=red size=-0.3><b>*</b></font>, Trang Nguyen<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>.</p>
</li>
</ul>
<h2 style="color:steelblue;">2023</h2>
<hr>
<ul>
<li><p> [64]<a href="https://arxiv.org/pdf/2005.11411.pdf"><font color=blue size=+0.3> Instability, computational efficiency, and statistical accuracy </font></a>. <i>Accepted with 
minor revision at Journal of Machine Learning Research (JMLR), 2023</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p>[63] <a href="SDE_Arxiv_submission_v2.pdf"><font color=blue size=+0.3> A diffusion process perspective on the posterior contraction rates for parameters</font></a>. <i> Accepted with minor revision at SIAM Journal on Mathematics of Data Science (SIMODS), 2023</i>.
<br />
Wenlong Mou, <b>Nhat Ho</b>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p> [62] <a href="https://arxiv.org/pdf/1811.02657.pdf"><font color=blue size=+0.3> A Bayesian perspective of convolutional neural networks through a deconvolutional generative model</font></a>. <i>Journal of Machine Learning Research (JMLR), 2023</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Ankit Patel, Anima Anandkumar, Michael I. Jordan, Richard Baraniuk. </p>
</li>
</ul>
<ul>
<li><p>[61] <a href="Softmax_MoE.pdf"><font color=blue size=+0.3> Demystifying softmax gating in Gaussian mixture of
experts </font></a>. <i> Advances in NeurIPS, 2023 (Spotlight)</i>. 
<br />
Huy Nguyen, Tin Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[60] <a href="Energy_sliced.pdf"><font color=blue size=+0.3> Energy-based sliced Wasserstein distance </font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[59] <a href="https://arxiv.org/pdf/2301.11808.pdf"><font color=blue size=+0.3> Minimax optimal rate for parameter estimation in multivariate deviated models</font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Dat Do<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[58] <a href="https://arxiv.org/pdf/2301.03749.pdf"><font color=blue size=+0.3> Markovian sliced Wasserstein distances: Beyond
independent projections </font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Khai Nguyen, Tongzheng Ren, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[57] <a href="https://arxiv.org/pdf/2210.05794.pdf"><font color=blue size=+0.3> Designing robust transformers using robust kernel density estimation</font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[56] <a href="https://arxiv.org/pdf/2306.11925.pdf"><font color=blue size=+0.3> LVM-Med: Learning large-Scale self-Supervised vision models for medical imaging via second-order graph matching</font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Duy Nguyen, Hoang Nguyen, Nghiem Diep, Tan Pham, Tri Cao, Binh T.  Nguyen, Paul Swoboda, <b>Nhat Ho</b>, Shadi Albarqouni, Pengtao Xie, Mathias Niepert, Daniel Sonntag.</p>
</li>
</ul>
<ul>
<li><p>[55] <a href="https://arxiv.org/pdf/2301.11496.pdf"><font color=blue size=+0.3> On excess mass behavior in Gaussian mixture models with Orlicz Wasserstein distances </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Aritra Guha, <b>Nhat Ho</b>, Long Nguyen.</p>
</li>
</ul>
<ul>
<li><p>[54] <a href="https://arxiv.org/pdf/2301.00437.pdf"><font color=blue size=+0.3> Neural collapse in deep linear network: from balanced to imbalanced data </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Hien Dang<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Tho Tran, Stanley Osher, Hung Tran, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[53] <a href="https://arxiv.org/pdf/2301.04791.pdf"><font color=blue size=+0.3> Self-attention amortized distributional projection optimization for sliced
Wasserstein point-clouds reconstruction </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Khai Nguyen<font color=red size=-0.3><b>*</b></font>, Dang Nguyen<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[52] <a href="https://arxiv.org/pdf/2211.15779.pdf"><font color=blue size=+0.3> Revisiting over-smoothing and over-squashing using Ollivier's Ricci curvature </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Khang Nguyen, Tan Minh Nguyen, Nong Minh Hieu, Vinh Duc Nguyen, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p>[51] <a href="Hierarchical_SW.pdf"><font color=blue size=+0.3> Hierarchical sliced Wasserstein distance</font></a>. <i> ICLR, 2023</i>. 
<br />
Khai Nguyen, Tongzheng Ren, Huy Nguyen, Litu Rout, Tan Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [50] <a href="ICLR_Primal_Dual.pdf"><font color=blue size=+0.3> A primal-dual framework for transformers and neural networks</font></a>. <i> ICLR, 2023 (Spotlight)</i>. 
<br />
Tan Minh Nguyen, Tam Minh Nguyen, <b>Nhat Ho</b>, Andrea L. Bertozzi, Richard Baraniuk, Stanley Osher.</p>
</li>
</ul>
<ul>
<li><p>[49] <a href="GLOT_arxiv.pdf" ><font color=blue size=+0.3> Global-local regularization via distributional robustness</font></a>. <i> AISTATS, 2023</i>.
<br />
Hoang Phan, Trung Le, Trung Phung, Tuan Anh Bui, <b>Nhat Ho</b>, Dinh Phung.</p>
</li>
</ul>
<ul>
<li><p>[48] <a href="https://arxiv.org/pdf/2110.15538.pdf"><font color=blue size=+0.3> Model fusion of heterogeneous neural networks via cross-layer alignment </font></a>. <i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2023 (Top 3%)</i>. 
<br />
Dang Nguyen, Trang Nguyen, Khai Nguyen, Dinh Phung, Hung Bui, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p>[47] <a href="FiAK_Transformer.pdf"><font color=blue size=+0.3> A probabilistic framework for pruning transformers via a finite admixture of keys </font></a>. <i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2023 (Top 3%)</i>.
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Long Bui, Hai Do, Dung Le, Hung Tran-The, Khuong Nguyen, <b>Nhat Ho</b>, Stanley Osher, Richard Baraniuk.</p>
</li>
</ul>
<ul>
<li><p> [46]<a href="https://arxiv.org/pdf/2212.01893.pdf"> <font color=blue size=+0.3>  Joint self-supervised image-volume representation learning with intra-inter contrastive clustering </font></a>. <i> AAAI, 2023</i>. 
<br />
Duy Nguyen, Nguyen Hoang, Truong Mai, Cao Tri, Binh Nguyen, <b>Nhat Ho</b>, Paul Swoboda, Shadi Albarqouni, Pengtao Xie, Daniel Sonntag.</p>
</li>
</ul>
<ul>
<li><p> [45]<a href="HTS_Cluster_arxiv.pdf"<font color=blue size=+0.3> Efficient forecasting of large scale hierarchical time series via multilevel clustering </font></a>. <i> International conference on Time Series and Forecasting (ITISE), 2023</i>. 
<br />
Xing Han, Tongzheng Ren, Jing Hu, Joydeep Ghosh, <b>Nhat Ho</b>.</p>
</li>
</ul>
<h2 style="color:steelblue;">2022</h2>
<hr>
<ul>
<li><p> [44] <a href="Mixture_Experts_Arxiv.pdf"><font color=blue size=+0.3> Convergence rates for Gaussian mixtures of experts</font></a>. <i>Journal of Machine Learning Research (JMLR), 2022</i>.
<br />
<b>Nhat Ho</b>, Chiao-Yu Yang, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p> [43] <a href="https://arxiv.org/pdf/1906.01437.pdf"><font color=blue size=+0.3> On the efficiency of entropic regularized
algorithms for optimal transport </font></a>. <i> Journal of Machine Learning Research (JMLR), 2022</i>. 
<br />
Tianyi Lin, <b>Nhat Ho</b>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p> [42] <a href="https://arxiv.org/pdf/1910.00152.pdf"><font color=blue size=+0.3> On the complexity of approximating multi-marginal optimal transport</font></a>. <i>Journal of Machine Learning Research (JMLR), 2022</i>. 
<br />
Tianyi Lin<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Marco Cuturi, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p> [41] <a href="Bayesian_Consistency_Smooth_Final.pdf"><font color=blue size=+0.3> Bayesian consistency with the supremum metric </font></a>. <i> Statistica Sinica, 2022</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p> [40] <a href="Revisiting_Sliced_Wasserstein_Arxiv.pdf"><font color=blue size=+0.3> Revisiting projected Wasserstein metric on images: from vectorization to convolution</font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [39] <a href="Fourier_former.pdf"><font color=blue size=+0.3> FourierFormer: Transformer meets generalized Fourier integral attentions </font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Minh Pham<font color=red size=-0.3><b>*</b></font>, Tam Nguyen, Khai Nguyen, Stanley Osher, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [38] <a href="Amortized_Projection_Optimization_for_Mini_batch_Projected_Wasserstein.pdf"><font color=blue size=+0.3> Amortized projection optimization for sliced Wasserstein generative models</font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [37] <a href="ICML_FiSHformer.pdf"><font color=blue size=+0.3>  Improving Transformer with an admixture of attention heads </font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Hai Do, Khai Nguyen, Vishwanath Saragadam, Minh Pham, Khuong Nguyen,Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p> [36] <a href="v_arxiv.pdf"<font color=blue size=+0.3> Beyond black box densities: Parameter learning for the deviated components</font></a>. <i>Advances in NeurIPS, 2022</i>.
<br />
Dat Do<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Long Nguyen.</p>
</li>
</ul>
<ul>
<li><p> [35] <a href="https://arxiv.org/pdf/2206.01934.pdf"><font color=blue size=+0.3> Stochastic multiple target sampling gradient descent
</font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Hoang Phan, Ngoc Tran, Trung Le, Toan Tran, <b>Nhat Ho</b>, Dinh Phung.</p>
</li>
</ul>
<ul>
<li><p> [34] <a href="Refined_metric.pdf"<font color=blue size=+0.3> Refined convergence rates for maximum likelihood estimation under finite mixture models</font></a>. <i> Proceedings of the ICML, 2022 (Long Presentation)</i>. 
<br />
Tudor Manole, <b>Nhat Ho</b>.
</li>
</ul>
<ul>
<li><p> [33] <a href="https://arxiv.org/pdf/2110.08678.pdf"<font color=blue size=+0.3> Improving Transformers with probabilistic attention keys </font></a>. <i>Proceedings of the ICML, 2022</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Dung Le, Khuong Nguyen, Anh Tran, Richard Baraniuk, Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p> [32] <a href="https://arxiv.org/pdf/2108.10961.pdf"><font color=blue size=+0.3>  Entropic Gromov-Wasserstein between Gaussian distributions</font></a>. <i>Proceedings of the ICML, 2022</i>. 
<br />
Khang Le<font color=red size=-0.3><b>*</b></font>, Dung Le<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Dat Do, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [31] <a href="https://arxiv.org/pdf/2108.09645.pdf"><font color=blue size=+0.3>  Improving minibatch optimal transport via partial transportation</font></a>. <i>Proceedings of the ICML, 2022</i>. 
<br />
Khai Nguyen, Dang Nguyen, Anh Le, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [30] <a href="https://arxiv.org/pdf/2102.05912.pdf"><font color=blue size=+0.3> BoMb-OT: On batch of mini-batches optimal transport </font></a>. <i>Proceedings of the ICML, 2022</i>.
<br />
Khai Nguyen, Dang Nguyen, Quoc Nguyen, Tung Pham, Dinh Phung, Hung Bui, Trung Le, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [29] <a href="Architecture_Agnostic.pdf"<font color=blue size=+0.3> Architecture agnostic federated learning for neural networks</font></a>. <i> Proceedings of the ICML, 2022</i>.
<br />
Disha Makhija, Xing Han, <b>Nhat Ho</b>, Joydeep Ghosh.</p>
</li>
</ul>
<ul>
<li><p> [28] <a href="Polyak_Step_Size_V3.pdf"><font color=blue size=+0.3> Towards statistical and computational complexities of Polyak step size gradient descent </font></a>. <i>AISTATS, 2022</i>. 
<br />
Tongzheng Ren<font color=red size=-0.3><b>*</b></font>, Fuheng Cui<font color=red size=-0.3><b>*</b></font>, Alexia Atsidakou<font color=red size=-0.3><b>*</b></font>, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [27] <a href="https://arxiv.org/pdf/2108.07992.pdf"><font color=blue size=+0.3>  On multimarginal partial optimal transport: Equivalent forms and computational complexity</font></a>. <i>AISTATS, 2022</i>. 
<br />
Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Khang Le<font color=red size=-0.3><b>*</b></font>, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p> [26] <a href="Arxiv_TF.pdf"><font color=blue size=+0.3> On structured filtering-clustering: Global error bound and optimal first-order algorithms</font></a>. <i>AISTATS, 2022</i>. 
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Tianyi Lin<font color=red size=-0.3><b>*</b></font>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p> [25] <a href="Full_JASA.pdf"><font color=blue size=+0.3> Weak separation in mixture models and implications for principal stratification</font></a>. <i>AISTATS, 2022</i>.
<br /> 
Avi Feller<font color=red size=-0.3><b>**</b></font>, Evan Greif<font color=red size=-0.3><b>**</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Luke W. Miratrix<font color=red size=-0.3><b>**</b></font>, Natesh S.
Pillai<font color=red size=-0.3><b>**</b></font>. </p>
</li>
</ul>
<h2 style="color:steelblue;">2021</h2>
<hr>
<ul>
<li><p> [24] <a href="https://arxiv.org/pdf/1901.05078.pdf"><font color=blue size=+0.3> On posterior contraction of parameters and interpretability in Bayesian mixture modeling </font></a>. <i> Bernoulli 27 (4), 2159-2188, 2021</i>.
<br />
Aritra Guha, <b>Nhat Ho</b>, XuanLong Nguyen.</p>
</li>
</ul>
<ul>
<li><p> [23] <a href="https://arxiv.org/pdf/1909.08787.pdf"><font color=blue size=+0.3> On efficient multilevel clustering via Wasserstein distances</font></a>. <i>Journal of Machine Learning Research (JMLR), 22, 1-43, 2021</i>.
<br />
Viet Huynh<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Nhan Dam, XuanLong Nguyen, Mikhail Yurochkin, Hung Bui, Dinh Phung.</p>
</li>
</ul>
<ul>
<li><p> [22] <a href="https://arxiv.org/pdf/2102.06857.pdf"><font color=blue size=+0.3> On robust optimal transport: Computational complexity and barycenter computation </font></a>. <i>Advances in NeurIPS, 2021</i>.
<br />
Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Khang Le<font color=red size=-0.3><b>*</b></font>, Quang Nguyen, Tung Pham, Hung Bui<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p> [21] <a href="https://arxiv.org/pdf/2102.07927.pdf"><font color=blue size=+0.3> Structured Dropout variational inference for Bayesian neural networks </font></a>. <i>Advances in NeurIPS, 2021</i>.
<br />
Son Nguyen<font color=red size=-0.3><b>*</b></font>, Duong Nguyen<font color=red size=-0.3><b>*</b></font>, Khai Nguyen, Khoat Than, Hung Bui<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p> [20] <a href="LAMDA_v1.pdf"><font color=blue size=+0.3> LAMDA: Label matching deep domain adaptation</font></a>. <i>Proceedings of the ICML, 2021</i>.
<br />
Trung Le, Tuan Nguyen, <b>Nhat Ho</b>, Hung Bui, Dinh Phung.  </p>
</li>
</ul>
<ul>
<li><p> [19] <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Nguyen_Point-Set_Distances_for_Learning_Representations_of_3D_Point_Clouds_ICCV_2021_paper.pdf"><font color=blue size=+0.3> Point-set distances for learning representations of 3D point clouds</font></a>. <i>International Conference on Computer Vision (ICCV), 2021</i>.
<br />
Trung Nguyen, Hieu Pham, Tam Le, Tung Pham, <b>Nhat Ho</b>, Son Hua.  </p>
</li>
</ul>
<ul>
<li><p> [18] <a href="https://arxiv.org/pdf/2006.02601.pdf"><font color=blue size=+0.3> On the minimax optimality of the EM algorithm for learning two-component mixed linear regression</font></a>. <i> AISTATS, 2021</i>. 
<br />
Jeong Y. Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p> [17] <a href="https://arxiv.org/pdf/1910.04462.pdf"><font color=blue size=+0.3> Flow-based alignment approaches
for probability measures in different spaces</font></a>. <i>AISTATS, 2021</i>. 
<br />
Tam Le<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Makoto Yamada.</p>
</li>
</ul>
<ul>
<li><p> [16] <a href="https://arxiv.org/pdf/2002.07367.pdf"><font color=blue size=+0.3> Distributional sliced-Wasserstein and applications to deep generative modeling</font></a>. <i>ICLR, 2021 (Spotlight)</i>.
<br />
Khai Nguyen, <b>Nhat Ho</b>, Tung Pham, Hung Bui.  </p>
</li>
</ul>
<ul>
<li><p> [15] <a href="https://arxiv.org/pdf/2010.01787.pdf"><font color=blue size=+0.3> Improving relational regularized autoencoders with spherical sliced fused Gromov Wasserstein</font></a>. <i>ICLR, 2021</i>.
<br />
Khai Nguyen, Son Nguyen, <b>Nhat Ho</b>, Tung Pham, Hung Bui.  </p>
</li>
</ul>
<h2 style="color:steelblue;">2020</h2>
<hr>
<ul>	
<li><p> [14] <a href="https://arxiv.org/pdf/1810.00828.pdf"><font color=blue size=+0.3> Singularity,
misspecification, and the convergence rate of EM</font></a>. <i>Annals of Statistics, 48(6), 3161-3182, 2020</i>. 
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p> [13] <a href="https://arxiv.org/pdf/1709.08094.pdf"><font color=blue size=+0.3> Robust estimation of mixing measures in finite mixture models</font></a>. <i>Bernoulli, 26(2), 828-857, 2020</i>.
<br />
<b>Nhat Ho</b>, XuanLong Nguyen, Ya'acov Ritov.</p>
</li>
</ul>
<ul>
<li><p> [12] <a href="https://arxiv.org/pdf/2006.07458.pdf"><font color=blue size=+0.3> Projection robust Wasserstein distance and Riemannian optimization </font></a>. <i> Advances in NeurIPS, 2020 (Spotlight)</i>. 
<br />
Tianyi Lin<font color=red size=-0.3><b>*</b></font>, Chenyou Fan<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>, Marco Cuturi, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p> [11] <a href="https://arxiv.org/pdf/2002.04783.pdf"><font color=blue size=+0.3> Fixed-support Wasserstein barycenters:
computational hardness and fast algorithm </font></a>. <i> Advances in NeurIPS, 2020</i>. 
<br />
Tianyi Lin, <b>Nhat Ho</b>, Xi Chen, Marco Cuturi, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p> [10] <a href="https://arxiv.org/pdf/2002.03293.pdf"><font color=blue size=+0.3> On unbalanced optimal transport: an analysis of Sinkhorn algorithm</font></a>. <i>Proceedings of the ICML, 2020</i>. 
<br />
Khiem Pham<font color=red size=-0.3><b>*</b></font>, Khang Le<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>, Tung Pham, Hung Bui.</p>
</li>
</ul>
<ul>
<li><p> [9] <a href="https://arxiv.org/pdf/1902.00194.pdf"><font color=blue size=+0.3> Sharp analysis of Expectation-Maximization for weakly identifiable models</font></a>. <i>AISTATS, 2020</i>.
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p> [8] <a href="https://arxiv.org/pdf/1905.09952.pdf"><font color=blue size=+0.3> Fast algorithms for computational optimal transport and Wasserstein barycenter</font></a>. <i>AISTATS, 2020</i>. 
<br />
Wenshuo Guo, <b>Nhat Ho</b>, Michael I. Jordan. </p>
</li>
</ul>
<h2 style="color:steelblue;"> Before 2020</h2>
<hr>
<ul>
<li><p> [7] <a href="AoS_2016.pdf"><font color=blue size=+0.3>Convergence rates of parameter estimation for some weakly identifiable finite mixtures</font></a>. <i>Annals of Statistics, 44(6), 2726-2755, 2016</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen. </p>
</li>
</ul>
<ul>
<li><p> [6] <a href="https://arxiv.org/pdf/1609.02655.pdf"><font color=blue size=+0.3> Singularity structures and impacts on parameter estimation behavior in finite mixtures of distributions</font></a>. <i>SIAM Journal on Mathematics of Data Science (SIMODS), 1(4), 730–758, 2019</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen.  </p>
</li>
</ul>
<ul>
<li><p> [5] <a href="https://arxiv.org/pdf/1901.06482.pdf"><font color=blue size=+0.3> On efficient optimal transport: an analysis of greedy and accelerated mirror descent algorithms</font></a>. <i>Proceedings of the ICML, 2019</i>.
<br />
Tianyi Lin<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p> [4] <a href="Ejs_2016.pdf"><font color=blue size=+0.3>On strong identifiability and convergence rates of
parameter estimation in finite mixtures</font></a>. <i>Electronic Journal of Statistics, 10(1), 271-307, 2016</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen.</p>
</li>
</ul>
<ul>
<li><p> [3] <a href="https://arxiv.org/pdf/1810.11911.pdf"><font color=blue size=+0.3> Probabilistic multilevel clustering via composite transportation distance</font></a>. <i>AISTATS, 2019</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Viet Huynh<font color=red size=-0.3><b>*</b></font>, Dinh Phung, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p> [2] <a href="EM_misspecified_unified.pdf"><font color=blue size=+0.3> Theoretical guarantees for EM under misspecified Gaussian mixture models </font></a>. <i>Advances in NeurIPS, 2018</i>.
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p> [1] <a href="https://arxiv.org/pdf/1706.03883.pdf"><font color=blue size=+0.3> Multilevel clustering via Wasserstein means</font></a>. <i>Proceedings of the ICML, 2017</i>.
<br />
<b>Nhat Ho</b>, XuanLong Nguyen, Mikhail Yurochkin, Hung Hai Bui, Viet Huynh, and Dinh Phung.</p>
</li>
</ul>
<h2 style="color:steelblue;"> Phd Thesis/ Manuscripts</h2>
<ul>
<li><p><a href="https://deepblue.lib.umich.edu/bitstream/handle/2027.42/140958/minhnhat_1.pdf?sequence=1&isAllowed=y"><font color=blue size=+0.3> Parameter Estimation and Multilevel Clustering with Mixture and Hierarchical Models </font></a>. <i>Phd Thesis, University of Michigan, Ann Arbor</i>. Advisors: Professor <a href="http://dept.stat.lsa.umich.edu/~xuanlong/">Long Nguyen</a> and Professor <a href="http://www-personal.umich.edu/~yritov/jr.html">Ya'acov Ritov</a>.
</li>
</ul>
<hr>
<div id="footer">
<div id="footer-text">
Page generated 2020-08-10 20:31:45 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
