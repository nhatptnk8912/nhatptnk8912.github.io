<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><br /></div>
<div class="menu-category"><b>Nhat Ho</b></div>
<hr>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<br>
<div class="menu-category"><b>Research</b></div>
<hr>
<div class="menu-item"><a href="publications_type.html">Publications&nbsp;(by&nbsp;type)</a></div>
<div class="menu-item"><a href="publications_topics.html">Publications&nbsp;(by&nbsp;topic)</a></div>
<div class="menu-item"><a href="publications_year.html">Publications&nbsp;(by&nbsp;year)</a></div>
<div class="menu-item"><a href="collaborators.html">Students/&nbsp;Collaborators</a></div>
<div class="menu-item"><a href="workshops.html">Workshops/&nbsp;Talks</a></div>
</td>
<td id="layout-content">
<h1 style="color:steelblue;">Nhat Ho</h1>
<table class="imgtable"><tr><td>
<img src="Nhat_Ho_Image.jpeg" alt="Coming Soon!" width="250px" height="292px" />&nbsp;</td>
<td align="left"><p>Assistant Professor <br />
<a href="https://stat.utexas.edu/">Department of Statistics and Data Sciences</a> <br />
<a href="https://www.utexas.edu/">The University of Texas at Austin</a> <br /></p>
<p>Other affiliations: <br />
Core member, <a href="https://ml.utexas.edu/about">Machine Learning Laboratory</a> <br /></p>
<p>Email: <a href="mailto:minhnhat@utexas.edu">minhnhat@utexas.edu</a> <br />
Office: WEL 5.242, 105 E 24th Street Austin, TX 78712 </p>
</td></tr></table>
<h2 style="color:steelblue;">Brief Biography</h2>
<hr>
<p>I am currently an Assistant Professor of Statistics and Data Sciences at the University of Texas at Austin. I am also a core member of the <a href="https://ml.utexas.edu/about">Machine Learning Laboratory</a> and senior personnel of the <a href="https://www.ifml.institute/">Institute for Foundations of Machine Learning (IFML)</a>. Before going to Austin, I was a postdoctoral fellow in the Electrical Engineering and Computer Science (EECS) Department where I am very fortunate to be mentored by Professor <a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a> and Professor <a href="http://people.eecs.berkeley.edu/~wainwrig/">Martin J. Wainwright</a>. Going further back in time, I finished my Phd degree in 2017 at the Department of Statistics, University of Michigan, Ann Arbor where I am very fortunate to be advised by Professor <a href="http://dept.stat.lsa.umich.edu/~xuanlong/">Long Nguyen</a> and Professor <a href="http://www-personal.umich.edu/~yritov/jr.html">Ya'acov Ritov</a>.</p>
<h2 style="color:steelblue;">Research interests</h2>
<hr>
<p> A central theme of my research focuses on four important aspects of complex and large-scale models and data: 
<ul>
<li><p>(1) Interpretability, Efficiency, and Robustness of deep learning and complex machine learning models, including  Transformer architectures, Deep Generative Models, Convolutional Neural Networks, etc.;</p> </li>

<li><p>(2) Scalability of Optimal Transport for machine learning and deep learning applications;</p> </li> 

<li><p>(3) Stability and Optimality of optimization and sampling algorithms for solving complex statistical machine learning models; </p> </li> 

<li><p>(4) Heterogeneity of complex data, including Mixture and Hierarchical Models, Bayesian Nonparametrics, etc.</p> </li>
</ul>

<p> For the first aspect (1), an example of our works (<a href="https://arxiv.org/pdf/2110.08678.pdf"><font color=blue size=+0.3>T.1</font></a>, <a href="FiSHformer_Final.pdf"><font color=blue size=+0.3>T.2</font></a>) includes using mixture and hierarchical models to improve the redundancy in Transformer, a recent state-of-the-art deep learning archiecture for language and computer vision applications. Furthermore, we also utilize Fourier Integral Theorem and its generalized version (<a href="FourierFormer_Final.pdf"><font color=blue size=+0.3>T.3</font></a>), a beautiful result in mathematics, to improve the interpretability and performance of Transformer. The Fourier Integral Theorem is also used in our other works to build estimators in other machine learning and statistics applications (<a href="JMLR_V1.pdf"><font color=blue size=+0.3>T.4</font></a>). Finally, we also develop a Bayesian deconvolution model (<a href="https://arxiv.org/pdf/1811.02657.pdf"><font color=blue size=+0.3>T.5</font></a>) to understand Convolutional Neural Networks.</p>

<p> For the second aspect (2), we focus on improving the scalability and curse of dimensionality of optimal transport in deep learning applications, such as deep generative model, domain adaptation, etc. For the curse of dimensionality, we propose several new variants of sliced optimal transport (<a href="Revisiting_Sliced_Wasserstein_Arxiv.pdf"><font color=blue size=+0.3>OT.1</font></a>, <a href="Amortized_Projection_Optimization_for_Mini_batch_Projected_Wasserstein.pdf"><font color=blue size=+0.3>OT.2</font></a>) to not only circumvent the curse of dimensionality of optimal transport but also improve the sampling scheme and training procedure of sliced optimal transport to select the most important directions. For the scalability, we propose new minibatch frameworks (<a href="https://arxiv.org/pdf/2102.05912.pdf"><font color=blue size=+0.3>OT.3</font></a>, <a href="https://arxiv.org/pdf/2102.05912.pdf"><font color=blue size=+0.3>OT.4</font></a>) to improve the misspecified matching issues of the current minibatch optimal transport in the literature. Furthermore, we also develop several optimization algorithms with near optimal computational complexities (<a href="https://arxiv.org/pdf/1906.01437.pdf"><font color=blue size=+0.3>OT.5</font></a>, <a href="https://arxiv.org/pdf/2102.06857.pdf"><font color=blue size=+0.3>OT.6</font></a>, <a href="https://arxiv.org/pdf/2006.07458.pdf"><font color=blue size=+0.3>OT.7</font></a>) for approximating the optimal transport and its variants.</p>

<p> For the third aspect (3), we study the interplay and trade-off between the instability, statistical accuracy, and computational efficiency of optimization and sampling algorithms (<a href="https://arxiv.org/pdf/2005.11411.pdf"><font color=blue size=+0.3>O.1</font></a>) for solving parameter estimation in statistical machine learning models. Based on these insights, we provide a rigorous statistical behaviors of the Expectation-Maximization (EM) (<a href="https://arxiv.org/pdf/1810.00828.pdf"><font color=blue size=+0.3>O.2</font></a>, <a href="https://arxiv.org/pdf/1902.00194.pdf"><font color=blue size=+0.3>O.3</font></a>) algorithm for solving mixture models and of the factorized gradient descent for solving a class of low-rank matrix factorization problems (<a href="https://arxiv.org/pdf/2102.02756.pdf"><font color=blue size=+0.3>O.4</font></a>). Finally, in the recent work, we propose the exponential schedule for gradient descent (<a href="Short_Arxiv_version.pdf"><font color=blue size=+0.3>O.5</font></a>) and demonstrate that this algorithm obtains the optimal linear computational complexity for solving parameter estimation in statistical machine learning models.</p> 

<p> For the fourth aspect (4), an example of our works includes the statistical and geometric behaviors of latent variables in mixture and hierarchical model via tools from optimal transport, quantization theory, and algebraic geometry. For example, we demonstrate that the convergence rates of the maximum likelihood estimation for finite mixture models and finite mixture of experts are determined by the solvability of some system of the polynomial equations, which is one of the key problems in algebraic geometry (<a href="Refined_metric.pdf"><font color=blue size=+0.3>M.1</font></a>, <a href="Mixture_Experts_Arxiv.pdf"><font color=blue size=+0.3>M.2</font></a>, <a href="https://arxiv.org/pdf/1609.02655.pdf"><font color=blue size=+0.3>M.3</font></a>, <a href="AoS_2016.pdf"><font color=blue size=+0.3>M.4</font></a>). These theories with the convergence rates of the MLE also lead to a novel model selection procedure (<a href="https://arxiv.org/pdf/1901.05078.pdf"><font color=blue size=+0.3>M.5</font></a>) in finite and infinite mixture models.</p>

<p> Apart from these topics, we also study Bayesian inference and asymptotics from new perspectives. For example, we utilize diffusion process to establish the posterior convergence rate of parameters in statistical models (<a href="SDE_Arxiv_submission_v2.pdf"><font color=blue size=+0.3>E.1</font></a>) or employ Fourier Integral Theorem to establish the posterior consistency of Bayesian nonparametric models (<a href="Bayesian_Consistency_Smooth_Final.pdf"><font color=blue size=+0.3>E.2</font></a>). 

<h2 style="color:steelblue;">Codes</h2>
<hr>
<p>The official Github link for codes of research papers from our Data Science and Machine Learning (DSML) Lab is: <a href= "https://github.com/UT-Austin-Data-Science-Group">https://github.com/UT-Austin-Data-Science-Group</a>.</p>
<h2 style="color:steelblue;"> Recent News</h2>
<hr style="margin-bottom:0cm;">
<ul>
<li><p>[06/2022] The paper "<a href="https://arxiv.org/pdf/1907.04377.pdf"><font color=blue size=+0.3> Convergence rates for Gaussian mixtures of experts </font></a>" ,coauthored with Chiao-Yu Yang and Michael I. Jordan, was accepted to Journal of Machine Learning Research (JMLR) under minor revision</p>
</li>
<li><p>[05/2022] Six papers (<a href="Refined_metric.pdf"><font color=blue size=+0.3>1</font></a>, <a href="https://arxiv.org/pdf/2110.08678.pdf"><font color=blue size=+0.3>2</font></a>, <a href="https://arxiv.org/pdf/2108.10961.pdf"><font color=blue size=+0.3>3</font></a>, <a href="https://arxiv.org/pdf/2108.09645.pdf"><font color=blue size=+0.3>4</font></a>, <a href="https://arxiv.org/pdf/2102.05912.pdf"><font color=blue size=+0.3>5</font></a>, <a href="Architecture_Agnostic.pdf"><font color=blue size=+0.3>6</font></a>) were accepted to International Conference on Machine Learning (ICML), 2022</p>
</li>
<li><p>[05/2022] The paper "<a href="https://arxiv.org/pdf/1906.01437.pdf"><font color=blue size=+0.3> On the efficiency of entropic regularized
algorithms for optimal transport </font></a>" ,coauthored with Tianyi Lin and Michael I. Jordan, was accepted to Journal of Machine Learning Research (JMLR), 2022</p>
</li>
<li><p>[02/2022] The paper "<a href="https://arxiv.org/pdf/1910.00152.pdf"><font color=blue size=+0.3> On the complexity of approximating multi-marginal optimal transport</font></a>" ,coauthored with Tianyi Lin, Marco Cuturi, and Michael I. Jordan, was accepted to Journal of Machine Learning Research (JMLR), 2022</p>
</li>
<li><p>[01/2022] Four papers (<a href="Polyak_Step_Size_V3.pdf"><font color=blue size=+0.3>1</font></a>, <a href="https://arxiv.org/pdf/2108.07992.pdf"><font color=blue size=+0.3>2</font></a>, <a href="Trend_Filtering_AISTATS.pdf"><font color=blue size=+0.3>3</font></a>, <a href="Weak_Separation_AISTATS.pdf"><font color=blue size=+0.3>4</font></a>) were accepted to International Conference on Artificial Intelligence and Statistics (AISTATS), 2022</p>
</li>
</ul>

<h2 style="color:steelblue;"> Selected Publications on Theory (Hierarchical and Mixture Models, (Non)-Convex Optimization, Optimal Transport, (Approximate) Bayesian Inference, etc.)</h2>
<hr style="margin-bottom:0cm;">
<ul>
<li><p><a href="https://arxiv.org/pdf/2005.11411.pdf"><font color=blue size=+0.3> Instability, computational efficiency, and statistical accuracy </font></a>. <i>Under revision</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p><a href="JMLR_V1.pdf"><font color=blue size=+0.3> Multivariate smoothing via the Fourier integral theorem and Fourier kernel </font></a>. <i> Under revision</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="SDE_Arxiv_submission_v2.pdf"><font color=blue size=+0.3> A diffusion process perspective on the posterior contraction rates for parameters</font></a>. <i>Under revision</i>.
<br />
Wenlong Mou<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="Bayesian_Consistency_Smooth_Final.pdf"><font color=blue size=+0.3> Bayesian consistency with the supremum metric </font></a>. <i> Under revision</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="Short_Arxiv_version.pdf"><font color=blue size=+0.3> An exponentially increasing step-size for parameter estimation in statistical models</font></a>. <i> Under review</i>. 
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Tongzheng Ren<font color=red size=-0.3><b>**</b></font>, Purnamrita Sarkar<font color=red size=-0.3><b>**</b></font>, Sujay Sanghavi<font color=red size=-0.3><b>**</b></font>, Rachel Ward<font color=red size=-0.3><b>**</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="v_arxiv.pdf"<font color=blue size=+0.3> Beyond black box densities: Parameter learning for the deviated components</font></a>. <i>Under review</i>.
<br />
Dat Do<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Long Nguyen.</p>
</li>
</ul>
<ul>
<li><p><a href="Refined_metric.pdf"<font color=blue size=+0.3> Refined convergence rates for maximum likelihood estimation under finite mixture models</font></a>. <i> Proceedings of the ICML, 2022 (Long Presentation)</i>. 
<br />
Tudor Manole, <b>Nhat Ho</b>.
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2108.10961.pdf"><font color=blue size=+0.3>  Entropic Gromov-Wasserstein between Gaussian distributions</font></a>. <i>Proceedings of the ICML, 2022</i>. 
<br />
Khang Le<font color=red size=-0.3><b>*</b></font>, Dung Le<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Dat Do, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Polyak_Step_Size_V3.pdf"><font color=blue size=+0.3> Towards statistical and computational complexities of Polyak step size gradient descent </font></a>. <i>AISTATS, 2022</i>. 
<br />
Tongzheng Ren<font color=red size=-0.3><b>*</b></font>, Fuheng Cui<font color=red size=-0.3><b>*</b></font>, Alexia Atsidakou<font color=red size=-0.3><b>*</b></font>, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.02601.pdf"><font color=blue size=+0.3> On the minimax optimality of the EM algorithm for learning two-component mixed linear regression</font></a>. <i>AISTATS, 2021</i>. 
<br />
Jeong Y. Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p><a href="Linear_scale_location.pdf"><font color=blue size=+0.3> Beyond EM algorithm on over-specified two-component location-scale Gaussian mixtures</font></a>. <i> Under review</i>. 
<br />
Tongzheng Ren<font color=red size=-0.3><b>*</b></font>, Fuheng Cui<font color=red size=-0.3><b>*</b></font>, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1906.01437.pdf"><font color=blue size=+0.3> On the efficiency of entropic regularized
algorithms for optimal transport </font></a>. <i> Journal of Machine Learning Research (JMLR), 2022</i>. 
<br />
Tianyi Lin, <b>Nhat Ho</b>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="Mixture_Experts_Arxiv.pdf"><font color=blue size=+0.3> Convergence rates for Gaussian mixtures of experts</font></a>. <i>Journal of Machine Learning Research (JMLR) (Accept Under Minor Revision)</i>.
<br />
<b>Nhat Ho</b>, Chiao-Yu Yang, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2102.02756.pdf"><font color=blue size=+0.3> On the computational and statistical complexity of over-parameterized matrix sensing </font></a>. <i>Under revision</i>.
<br />
Jiacheng Zhuo, Jeongyeol Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.07458.pdf"><font color=blue size=+0.3> Projection robust Wasserstein distance and Riemannian optimization </font></a>. <i> Advances in NeurIPS, 2020 (Spotlight)</i>. 
<br />
Tianyi Lin<font color=red size=-0.3><b>*</b></font>, Chenyou Fan<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>, Marco Cuturi, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p><a href="Normalized_GD.pdf"<font color=blue size=+0.3> Improving computational complexity in statistical models with second-order information </font></a>. <i> Under review</i>. 
<br />
Tongzheng Ren, Jiacheng Zhuo, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1901.05078.pdf"><font color=blue size=+0.3> On posterior contraction of parameters and interpretability in Bayesian mixture modeling </font></a>. <i> Bernoulli 27 (4), 2159-2188, 2021</i>.
<br />
Aritra Guha, <b>Nhat Ho</b>, XuanLong Nguyen.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1810.00828.pdf"><font color=blue size=+0.3> Singularity,
misspecification, and the convergence rate of EM</font></a>. <i>Annals of Statistics, 48(6), 3161-3182, 2020</i>. 
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1609.02655.pdf"><font color=blue size=+0.3> Singularity structures and impacts on parameter estimation behavior in finite mixtures of distributions</font></a>. <i>SIAM Journal on Mathematics of Data Science (SIMODS), 1(4), 730â€“758, 2019</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen.</p>
</li>
</ul>
<ul>
<li><p><a href="AoS_2016.pdf"><font color=blue size=+0.3>Convergence rates of parameter estimation for some weakly identifiable finite mixtures</font></a>. <i>Annals of Statistics, 44(6), 2726-2755, 2016</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen</p>
</li>
</ul>
<h2 style="color:steelblue;"> Selected Publications on Method and Application (Generative Models, Optimal Transport, Transformer, Convolutional Neural Networks, etc.)</h2>
<hr style="margin-bottom:0cm;">
<ul>
<li><p><a href="https://arxiv.org/pdf/1811.02657.pdf"><font color=blue size=+0.3> A Bayesian perspective of convolutional neural networks through a deconvolutional generative model</font></a>. <i>Under revision</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Ankit Patel, Anima Anandkumar, Michael I. Jordan, Richard Baraniuk. </p>
</li>
</ul>
<ul>
<li><p><a href="Revisiting_Sliced_Wasserstein_Arxiv.pdf"><font color=blue size=+0.3> Revisiting projected Wasserstein metric on images: from vectorization to convolution</font></a>. <i> Under review</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Fourier_former.pdf"><font color=blue size=+0.3> Transformer with Fourier integral attentions </font></a>. <i> Under review</i>. 
<br />
Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Minh Pham<font color=red size=-0.3><b>*</b></font>, Tam Nguyen, Khai Nguyen, Stanley Osher, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2110.08678.pdf"<font color=blue size=+0.3> Improving Transformers with probabilistic attention keys </font></a>. <i>ICML, 2022</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Dung Le, Khuong Nguyen, Anh Tran, Richard Baraniuk, Stanley Osher<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="Amortized_Projection_Optimization_for_Mini_batch_Projected_Wasserstein.pdf"><font color=blue size=+0.3> Amortized projection optimization for sliced Wasserstein generative models</font></a>. <i> Under review</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Statistical_Fourier_Theorem.pdf"><font color=blue size=+0.3> Statistical analysis using the Fourier integral theorem </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><font color=blue size=+0.3> FiSHFormer: Transformer with a finite admixture of shared heads </font></a>. <i> Under review</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Hai Do, Khai Nguyen, Vishwanath Saragadam, Minh Pham, Khuong Nguyen,Stanley Osher<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.06448.pdf"><font color=blue size=+0.3> Probabilistic best subset selection via gradient-based optimization</font></a>. <i>Under review</i>. 
<br />
Mingzhang Yin, <b>Nhat Ho</b>, Bowei Yan, Xiaoning Qian, Mingyuan Zhou.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2020-08-19 03:32:40 CDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
