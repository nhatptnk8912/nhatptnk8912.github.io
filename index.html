<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><br /></div>
<div class="menu-category"><b>Nhat Ho</b></div>
<hr>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<br>
<div class="menu-category"><b>Research</b></div>
<hr>
<div class="menu-item"><a href="publications_type.html">Publications&nbsp;(by&nbsp;type)</a></div>
<div class="menu-item"><a href="publications_topics.html">Publications&nbsp;(by&nbsp;topic)</a></div>
<div class="menu-item"><a href="publications_year.html">Publications&nbsp;(by&nbsp;year)</a></div>
<div class="menu-item"><a href="collaborators.html">Students/&nbsp;Collaborators</a></div>
<div class="menu-item"><a href="workshops.html">Workshops/&nbsp;Talks/&nbsp;Lectures</a></div>
</td>
<td id="layout-content">
<h1 style="color:steelblue;">Nhat Ho</h1>
<table class="imgtable"><tr><td>
<img src="Nhat_Ho_Profile.jpg" alt="Coming Soon!" width="250px" height="292px" />&nbsp;</td>
<td align="left"><p>Assistant Professor <br />
<a href="https://stat.utexas.edu/">Department of Statistics and Data Sciences</a> <br />
<a href="https://www.utexas.edu/">The University of Texas at Austin</a> <br /></p>
<p>Other affiliations: <br />
Core member, <a href="https://ml.utexas.edu/about">Machine Learning Laboratory</a> <br />
Senior personnel, <a href="https://www.ifml.institute/">Institute for Foundations of Machine Learning (IFML)</a> <br /></p>
<p>Email: <a href="mailto:minhnhat@utexas.edu">minhnhat@utexas.edu</a> <br />
Office: WEL 5.242, 105 E 24th Street Austin, TX 78712 </p>
</td></tr></table>
<h2 style="color:steelblue;">Brief Biography</h2>
<hr>
<p>I am currently an Assistant Professor of Statistics and Data Sciences at the University of Texas at Austin. I am also a core member of the <a href="https://ml.utexas.edu/about">Machine Learning Laboratory</a> and senior personnel of the <a href="https://www.ifml.institute/">Institute for Foundations of Machine Learning (IFML)</a>. Before going to Austin, I was a postdoctoral fellow in the Electrical Engineering and Computer Science (EECS) Department where I am very fortunate to be mentored by Professor <a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a> and Professor <a href="http://people.eecs.berkeley.edu/~wainwrig/">Martin J. Wainwright</a>. Going further back in time, I finished my Phd degree in 2017 at the Department of Statistics, University of Michigan, Ann Arbor where I am very fortunate to be advised by Professor <a href="http://dept.stat.lsa.umich.edu/~xuanlong/">Long Nguyen</a> and Professor <a href="http://www-personal.umich.edu/~yritov/jr.html">Ya'acov Ritov</a>.</p>
<h2 style="color:steelblue;">Research Interests</h2>
<hr>
<p> A central theme of my research focuses on four important aspects of complex and large-scale models and data: 
<ul>
<li><p>(1) Heterogeneity of complex data, including Mixture and Hierarchical Models, Bayesian Nonparametrics, etc.;</p> </li>	
	
<li><p>(2) Interpretability, Efficiency, Scalability, and Robustness of deep learning and complex machine learning models, including  Transformer architectures, Deep Generative Models, Convolutional Neural Networks, etc.;</p> </li>

<li><p>(3) Scalability and Efficiency of Optimal Transport for machine learning and deep learning applications;</p> </li> 

<li><p>(4) Stability, Optimality, and Robustness of optimization and sampling algorithms for solving complex statistical machine learning models. </p> </li> 
</ul>

<p> --- For the first aspect (1), an example of our works includes the statistical and geometric behaviors of latent variables in sparse and high dimensional mixture and hierarchical model via tools from optimal transport, quantization theory, and algebraic geometry. For example, we demonstrate that the convergence rates of the maximum likelihood estimation for finite mixture models and input-independent gating mixture of experts are determined by the solvability of some system of the polynomial equations, which is one of the key problems in algebraic geometry (<a href="Refined_metric.pdf"><font color=blue size=+0.3>M.1</font></a>, <a href="Mixture_Experts_Arxiv.pdf"><font color=blue size=+0.3>M.2</font></a>, <a href="https://arxiv.org/pdf/1609.02655.pdf"><font color=blue size=+0.3>M.3</font></a>, <a href="AoS_2016.pdf"><font color=blue size=+0.3>M.4</font></a>). These theories with the convergence rates of the MLE also lead to a novel model selection procedure (<a href="https://arxiv.org/pdf/1901.05078.pdf"><font color=blue size=+0.3>M.5</font></a>) in finite and infinite mixture models.</p>

Recently, we provided a comprehensive theory to long-standing open problem about parameter and expert estimation in softmax gating Gaussian mixture of experts (<a href="https://arxiv.org/pdf/2305.03288.pdf"><font color=blue size=+0.3>M.6</font></a>), a class of conditional mixtures being widely used in machine learning and deep learning to scale up large-scale neural networks architectures. Our theory relies on defining novel Voronoi-based losses among parameters, which can precisely capture the intrinsic interaction (via partial differential equations with respect to model parameters) between the softmax gating function and the expert functions. In a subsequent work (<a href="https://arxiv.org/abs/2402.02952"><font color=blue size=+0.3>M.7</font></a>), we also established general theories for softmax gating mixture of experts with least-square loss function. Furtherore, we also carried these insight into understanding several other important variants of softmax gating mixture of experts that have been currently used in scaling up Transformer and Large Language Model, including top-K sparse mixture of experts (<a href="https://arxiv.org/pdf/2309.13850.pdf"><font color=blue size=+0.3>M.8</font></a>), dense-to-sparse (equivalently temperature softmax) mixture of experts (<a href="https://arxiv.org/pdf/2401.13875.pd"><font color=blue size=+0.3>M.9</font></a>) and being used in other machine learning tasks (<a href="https://arxiv.org/pdf/2309.13850.pdf"><font color=blue size=+0.3>M.10</font></a>, <a href="https://arxiv.org/pdf/2305.07572.pdf"><font color=blue size=+0.3>M.11</font></a>). </p>

From the methodology and application sides, we recently developed novel effective training of sparse mixture of experts via competition for scaling up large-scale AI models (<a href="https://arxiv.org/abs/2402.02526"><font color=blue size=+0.3>M.12</font></a>) or utilized mixture of experts with Laplace gating function for Transformer, a recent state-of-the-art deep learning archiecture for language and computer vision applications, to develop large multimodal model for multimodal data appearing in eletronic health records (<a href="hhttps://arxiv.org/pdf/2402.03226.pdf"><font color=blue size=+0.3>M.13</font></a>).

<p> --- For the second aspect (2), we utilize insight from statistical machine learning modeling and theories, Hamilton-Jacobi partial differential equation (PDE) to understand deep learning and complex machine learning models. Examples of our works include using mixture and hierarchical models (<a href="https://arxiv.org/pdf/2110.08678.pdf"><font color=blue size=+0.3>T.1</font></a>, <a href="FiSHformer_Final.pdf"><font color=blue size=+0.3>T.2</font></a>) to improve the redundancy in Transformer or interpreting Transformer using primal-dual frameworks from support vector regression (<a href="ICLR_Primal_Dual.pdf"><font color=blue size=+0.3>T.3</font></a>). Furthermore, we also utilize Fourier Integral Theorem and its generalized version (<a href="FourierFormer_Final.pdf"><font color=blue size=+0.3>T.4</font></a>), a beautiful result in mathematics, to improve the interpretability and performance of Transformer. The Fourier Integral Theorem is also used in our other works to build estimators in other machine learning and statistics applications (<a href="JMLR_V1.pdf"><font color=blue size=+0.3>T.5</font></a>). Finally, we also develop a Bayesian deconvolution model (<a href="https://arxiv.org/pdf/1811.02657.pdf"><font color=blue size=+0.3>T.6</font></a>) to understand Convolutional Neural Networks or provide a complete theory for neural collapse phenomenon in deep linear neural network (<a href="https://arxiv.org/pdf/2301.00437.pdf"><font color=blue size=+0.3>T.7</font></a>).</p>

Recently, we also established guarantees for several interesting phenomena in training deep learning models, including Neural Collapse (<a href="https://arxiv.org/pdf/2301.00437.pdf"><font color=blue size=+0.3>T.8</font></a>, <a href="https://arxiv.org/abs/2401.02058"><font color=blue size=+0.3>T.9</font></a>), and Posterior Collapse (<a href="https://openreview.net/forum?id=4zZFGliCl9"><font color=blue size=+0.3>T.10</font></a>).

<p> --- For the third aspect (3), we focus on improving the scalability, efficiency, and curse of dimensionality of optimal transport in deep learning applications, such as deep generative model, domain adaptation, etc. For the efficiency and curse of dimensionality, we propose several new variants of sliced optimal transport (<a href="https://arxiv.org/pdf/2209.13570.pdf"><font color=blue size=+0.3>OT.1</font></a>, <a href="https://arxiv.org/pdf/2301.03749.pdf"><font color=blue size=+0.3>OT.2</font></a>, <a href="Revisiting_Sliced_Wasserstein_Arxiv.pdf"><font color=blue size=+0.3>OT.3</font></a>, <a href="Amortized_Projection_Optimization_for_Mini_batch_Projected_Wasserstein.pdf"><font color=blue size=+0.3>OT.4</font></a>, <a href="Energy_sliced.pdf"><font color=blue size=+0.3>OT.5</font></a>) to not only circumvent the curse of dimensionality of optimal transport but also improve the sampling scheme and training procedure of sliced optimal transport to select the most important directions. For the scalability, we propose new minibatch frameworks (<a href="https://arxiv.org/pdf/2108.09645.pdf"><font color=blue size=+0.3>OT.6</font></a>, <a href="https://arxiv.org/pdf/2102.05912.pdf"><font color=blue size=+0.3>OT.7</font></a>) to improve the misspecified matching issues of the current minibatch optimal transport in the literature. Furthermore, we also develop several optimization algorithms with near optimal computational complexities (<a href="https://arxiv.org/pdf/1906.01437.pdf"><font color=blue size=+0.3>OT.8</font></a>, <a href="https://arxiv.org/pdf/2102.06857.pdf"><font color=blue size=+0.3>OT.9</font></a>, <a href="https://arxiv.org/pdf/2006.07458.pdf"><font color=blue size=+0.3>OT.10</font></a>) for approximating the optimal transport and its variants.</p>

From the application sides, we proposed using (sliced) optimal transport and its variants for building large pretrained models for medical images (<a href="https://arxiv.org/pdf/2306.11925.pdf"><font color=blue size=+0.3>OT.11</font></a>), for audio-text retrieval (<a href="https://openreview.net/forum?id=l60EM8md3t"><font color=blue size=+0.3>OT.12</font></a>), for cortical surface reconstruction (<a href="https://openreview.net/forum?id=gxhRR8vUQb"><font color=blue size=+0.3>OT.13</font></a>), for molecular property prediction (<a href="https://arxiv.org/abs/2402.01975"><font color=blue size=+0.3>OT.14</font></a>), and for shape correspondence learning (<a href="OT_Shape_Correspondence.pdf"><font color=blue size=+0.3>OT.15</font></a>).

<p> --- For the fourth aspect (4), we study the interplay and trade-off between the instability, statistical accuracy, and computational efficiency of optimization and sampling algorithms (<a href="https://arxiv.org/pdf/2005.11411.pdf"><font color=blue size=+0.3>O.1</font></a>) for solving parameter estimation in statistical machine learning models. Based on these insights, we provide a rigorous statistical behaviors of the Expectation-Maximization (EM) (<a href="https://arxiv.org/pdf/1810.00828.pdf"><font color=blue size=+0.3>O.2</font></a>, <a href="https://arxiv.org/pdf/1902.00194.pdf"><font color=blue size=+0.3>O.3</font></a>) algorithm for solving mixture models and of the factorized gradient descent for solving a class of low-rank matrix factorization problems (<a href="https://arxiv.org/pdf/2102.02756.pdf"><font color=blue size=+0.3>O.4</font></a>). Finally, in the recent work, we propose the exponential schedule for gradient descent (<a href="Short_Arxiv_version.pdf"><font color=blue size=+0.3>O.5</font></a>) and demonstrate that this algorithm obtains the optimal linear computational complexity for solving parameter estimation in statistical machine learning models.</p> 

Recently, we proposed a novel robust criterion for distributionally robust optimization by combining insights from Bayesian nonparametric (e.g., Dirichlet Process) theory and recent decision-theoretic models of smooth ambiguity-averse preferences (<a href="https://arxiv.org/abs/2401.15771"><font color=blue size=+0.3>O.6</font></a>). 

<p> --- Apart from these topics, we also study Bayesian inference and asymptotics from new perspectives. For example, we utilize diffusion process to establish the posterior convergence rate of parameters in statistical models (<a href="SDE_Arxiv_submission_v2.pdf"><font color=blue size=+0.3>E.1</font></a>) or employ Fourier Integral Theorem to establish the posterior consistency of Bayesian nonparametric models (<a href="Bayesian_Consistency_Smooth_Final.pdf"><font color=blue size=+0.3>E.2</font></a>). 

<h2 style="color:steelblue;">Codes</h2>
<hr>
<p>The official Github link for codes of research papers from our Data Science and Machine Learning (DSML) Lab is: <a href= "https://github.com/UT-Austin-Data-Science-Group">https://github.com/UT-Austin-Data-Science-Group</a>.</p>
<h2 style="color:steelblue;">Editorial Boards of Journals</h2>
<hr style="margin-bottom:0cm;">
<ul>
<li><p><a href="https://imstat.org/journals-and-publications/electronic-journal-of-statistics/"><font color=blue size=+0.3> Electronic Journal of Statistics </font></a></p>
</li>
</ul>
<h2 style="color:steelblue;">Area Chairs of Conferences in Machine Learning and Artificial Intelligence</h2>
<hr style="margin-bottom:0cm;">
<ul>
<li><p><a href="https://icml.cc/"><font color=blue size=+0.3> International Conference on Machine Learning (ICML) </font></a></p>
</li>
</ul>
<ul>
<li><p><a href="https://iclr.cc/"><font color=blue size=+0.3> International Conference on Learning Representations (ICLR) </font></a></p>
</li>
</ul>
<ul>
<li><p><a href="https://aistats.org/aistats2023/"><font color=blue size=+0.3> International Conference on Artificial Intelligence and Statistics (AISTATS) </font></a></p>
</li>
</ul>
<ul>
<li><p><a href="https://aaai.org/aaai-conference/"><font color=blue size=+0.3> AAAI Conference on Artificial Intelligence (AAAI) </font></a></p>
</li>
</ul>
<h2 style="color:steelblue;">Media Coverage</h2>
<hr>
<p> Data Science, Machine Learning, Statistics, and Artifical Intelligence have become very important fields in Vietnam these days. However, as these fields are still very young in Vietnam, young Vietnamese generation often faces challenges to equip themselves with enough information, knowledge, and skills to pursue their career paths in these fields. For this reason, several leading newspapers and shows in Vietnam covered my path and story of becoming a professor in the leading US university as well as my opinion about these fields to inspire and provide necessary information to young generation in Vietnam that would like to pursue their careers in Data Science, Machine Learning, Statistics, and Artifical Intelligence, including:
<ul>
<li><p> Thanh Niên Newspaper (in Vietnamese):  <a href="
https://thanhnien.vn/giao-su-34-tuoi-giup-nguoi-tre-hoc-tien-si-tai-cac-truong-hang-dau-the-gioi-18524012319520191.htm"><font color=blue size=+0.3> Giáo sư 34 tuổi giúp người trẻ học tiến sĩ tại các trường hàng đầu thế giới</font></a></p>
</li> 
<li><p> Thanh Niên Newspaper (in Vietnamese):  <a href="
https://thanhnien.vn/nganh-nghe-cua-tuong-lai-nhieu-co-hoi-viec-lam-ve-khoa-hoc-du-lieu-185240513195048291.htm?"><font color=blue size=+0.3> Ngành nghề của tương lai: Nhiều cơ hội việc làm về khoa học dữ liệu</font></a></p>
</li> 
<li><p> Sài Gòn Giải Phóng Newspaper (in Vietnamese):  <a href="https://www.sggp.org.vn/nhung-canh-thien-di-cua-tri-tue-viet-post725895.html"><font color=blue size=+0.3> Báo Xuân Giáp Thìn "Những Cánh Thiên Di của Trí Tuệ Việt"</font></a>;  <a href="https://en.sggp.org.vn/outstanding-vietnamese-conquering-pinnacle-of-knowledge-overseas-post108043.html"><font color=blue size=+0.3> Outstanding Vietnamese conquering pinnacle of knowledge overseas</font></a></p>
</li> 
<li><p> Thanh Niên Newspaper (in Vietnamese):  <a href="
https://thanhnien.vn/viec-lam-linh-vuc-tri-tue-nhan-tao-co-hoi-viec-lam-rong-mo-185240111201218672.htm?"><font color=blue size=+0.3> Việc làm lĩnh vực trí tuệ nhân tạo: Cơ hội việc làm rộng mở</font></a></p>
</li>
<li><p> Tuổi Trẻ Newspaper (in Vietnamese):  <a href="
https://muctim.tuoitre.vn/muon-tro-thanh-ky-su-tri-tue-nhan-tao-can-hoc-tot-nhung-mon-nao-101240705150047831.htm?"><font color=blue size=+0.3> Muốn trở thành kỹ sư trí tuệ nhân tạo cần học tốt những môn nào?</font></a></p>
</li>
<li><p> Thanh Niên Newspaper (in Vietnamese):  <a href="
https://thanhnien.vn/trang-bi-ky-nang-su-dung-ai-de-co-them-nhieu-co-hoi-trong-cuoc-song-18524061419065217.htm"><font color=blue size=+0.3> Trang bị kỹ năng sử dụng AI để có thêm nhiều cơ hội trong cuộc sống</font></a>;  <a href="
https://thanhnien.vn/muon-su-dung-ai-bat-dau-tu-dau-185240613205028134.htm"><font color=blue size=+0.3> Muốn sử dụng AI, bắt đầu từ đâu?</font></a>; <a href="
https://thanhnien.vn/muon-su-dung-ai-bat-dau-tu-dau-nhung-nguoi-tre-khong-loi-nhip-185240616202521488.htm"><font color=blue size=+0.3> Muốn sử dụng AI, bắt đầu từ đâu?: Những người trẻ không... lỗi nhịp</font></a>; <a href="
https://thanhnien.vn/muon-su-dung-ai-bat-dau-tu-dau-nho-ai-khoi-nghiep-dot-pha-185240615185956215.htm"><font color=blue size=+0.3> Muốn sử dụng AI, bắt đầu từ đâu?: Nhờ AI, khởi nghiệp đột phá</font></a></p>
</li>
<li><p> Tiền Phong Newspaper (in Vietnamese): <a href="
https://svvn.tienphong.vn/chang-trai-bac-lieu-chia-se-hanh-trinh-tro-thanh-giao-su-tai-my-ve-ai-va-khoa-hoc-cong-nghe-post1574504.tpo"><font color=blue size=+0.3> Chàng trai Bạc Liêu chia sẻ hành trình trở thành giáo sư tại Mỹ về AI và Khoa học công nghệ</font></a></p>
</li>
<li><p> VnExpress Newspaper (in Vietnamese): <a href="
https://vnexpress.net/tu-hoc-sinh-tinh-le-thanh-giao-su-dai-hoc-my-4654827.html"><font color=blue size=+0.3> Từ học sinh tỉnh lẻ thành giáo sư đại học Mỹ</font></a></p>
</li>
<li><p> Thanh Niên Newspaper (in Vietnamese):  <a href="
https://thanhnien.vn/nguoi-tre-thieu-tram-trong-ky-nang-song-185231127195628766.htm?"><font color=blue size=+0.3> Người trẻ thiếu trầm trọng kỹ năng sống</font></a>;  <a href="
https://thanhnien.vn/nguoi-tre-thieu-tram-trong-ky-nang-song-nguy-co-tro-thanh-toi-pham-185231128200436214.htm?"><font color=blue size=+0.3> Người trẻ thiếu trầm trọng kỹ năng sống: Nguy cơ trở thành tội phạm</font></a>; <a href="
https://thanhnien.vn/nguoi-tre-thieu-tram-trong-ky-nang-song-5-cach-de-tu-lap-day-lo-hong-185231130200340149.htm?"><font color=blue size=+0.3> Người trẻ thiếu trầm trọng kỹ năng sống: 5 cách để tự lấp đầy 'lỗ hổng'</font></a></p>
</li>
<li><p> Sài Gòn Giải Phóng Newspaper (in Vietnamese):  <a href="
https://www.sggp.org.vn/di-vao-vung-chua-tung-kham-pha-post737383.html?"><font color=blue size=+0.3> Đi vào vùng chưa từng khám phá</font></a></p>
</li> 
<li><p> Giáo Dục Việt Nam Newspaper (in Vietnamese):  <a href="
https://giaoduc.net.vn/bai-hoc-kinh-nghiem-nhin-tu-cach-huy-dong-nguon-luc-tai-chinh-o-mot-so-dh-cua-my-post243483.gd"><font color=blue size=+0.3> Bài học kinh nghiệm nhìn từ cách huy động nguồn lực tài chính ở một số ĐH của Mỹ</font></a></p>
</li> 
<li><p> VTV1 Television Show "Cất Cánh" (in Vietnamese): <a href="
https://www.youtube.com/watch?v=_NX6gm_vPis&ab_channel=Th%E1%BB%8BnhChannel-K%C3%AAnhT%E1%BB%95ngh%E1%BB%A3p2"><font color=blue size=+0.3> Tiếng gọi quê hương</font></a></p>
</li>
<li><p> VTV1 Television Show "Toàn Cảnh Báo Xuân" (in Vietnamese): <a href="
https://vtv.vn/video/toan-canh-bao-xuan-09-02-2024-662344.htm"><font color=blue size=+0.3> Tự hào Trí Tuệ Việt</font></a></p>
</li>
</ul>
<h2 style="color:steelblue;"> Recent News</h2>
<hr style="margin-bottom:0cm;">
<ul>
<li><p>[01/2025] Four papers (<a href="https://arxiv.org/abs/2405.14131"><font color=blue size=+0.3>1</font></a>, <a href="https://arxiv.org/abs/2410.02200"><font color=blue size=+0.3>2</font></a>, <a href="https://arxiv.org/abs/2411.01123"><font color=blue size=+0.3>3</font></a>, <a href="https://arxiv.org/abs/2405.07482"><font color=blue size=+0.3>4</font></a>) were accepted to International Conference on Learning Representations (ICLR), 2025</p>
</li>
<li><p>[01/2025] One paper (<a href="https://arxiv.org/abs/2410.12258"><font color=blue size=+0.3>1</font></a>) was accepted to International Conference on Artificial Intelligence and Statistics (AISTATS), 2025</p>
</li>
<li><p>[11/2024] I am glad to serve as an Area Chair of the International Conference on Machine Learning (ICML), 2025</p>
</li>
<li><p>[09/2024] Six papers (<a href="https://arxiv.org/abs/2401.15771"><font color=blue size=+0.3>1</font></a>, <a href="https://arxiv.org/pdf/2402.03226.pdf"><font color=blue size=+0.3>2</font></a>, <a href="https://arxiv.org/abs/2405.13997"><font color=blue size=+0.3>3</font></a>, <a href="https://arxiv.org/abs/2405.14124"><font color=blue size=+0.3>4</font></a>, <a href="https://arxiv.org/abs/2404.15378"><font color=blue size=+0.3>5</font></a>, <a href="https://arxiv.org/abs/2306.07959"><font color=blue size=+0.3>6</font></a>) were accepted to Conference on Neural Information Processing Systems (NeurIPS), 2024</p>
</li>
<li><p>[09/2024] I am glad to serve as an Area Chair of the International Conference on Artificial Intelligence and Statistics (AISTATS), 2025</p>
</li>
<li><p>[08/2024] I am glad to serve as an Area Chair of the International Conference on Learning Representations (ICLR), 2025 and Senior Program Committee (Area Chair) of AAAI, 2025</p>
</li>
<li><p>[07/2024] The paper "<a href="Global_Convergence_EM_MoE.pdf"><font color=blue size=+0.3> Global optimality of the EM Algorithm for mixtures of two component linear regressions </font></a>" , coauthored with Jeongyeol Kwon, Wei Qian, Constantine Caramanis, Yudong Chen, Damek Davis was accepted to IEEE Transactions on Information Theory</p>
</li>	
<li><p>[05/2024] Seven papers (<a href="https://arxiv.org/pdf/2401.13875.pdf"><font color=blue size=+0.3>1</font></a>, <a href="https://arxiv.org/abs/2402.02952"><font color=blue size=+0.3>2</font></a>, <a href="https://arxiv.org/abs/2401.15889"><font color=blue size=+0.3>3</font></a>, <a href="https://arxiv.org/abs/2402.01975"><font color=blue size=+0.3>4</font></a>, <a href="https://arxiv.org/pdf/2309.13850.pdf"><font color=blue size=+0.3>5</font></a>, <a href="https://arxiv.org/abs/2401.02058"><font color=blue size=+0.3>6</font></a>, <a href="Normalized_GD.pdf"><font color=blue size=+0.3>7</font></a>) were accepted to International Conference on Machine Learning (ICML), 2024</p>
</li>
<li><p>[04/2024] The paper "<a href="Statistical_and_Computational_Complexities_of_BFGS.pdf"><font color=blue size=+0.3> Statistical and computational complexities of BFGS quasi-Newton method for generalized linear models </font></a>" , coauthored with Qiujiang Jin, Tongzheng Ren, Aryan Mokhtari was accepted to Transactions on Machine Learning Research (TMLR)</p>
</li>	
<li><p>[02/2024] The paper "<a href="https://arxiv.org/abs/2102.02756"><font color=blue size=+0.3> On the computational and statistical complexity of over-parameterized matrix sensing </font></a>" , coauthored with Jiacheng Zhuo, Jeongyeol Kwon, Constantine Caramanis was accepted to Journal of Machine Learning Research (JMLR)</p>
</li>
<li><p>[02/2024] The paper "<a href="main_version.pdf"><font color=blue size=+0.3> On integral theorems and their statistical properties </font></a>" , coauthored with Stephen Walker was accepted to Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</p>
</li>
<li><p>[02/2024] One paper (<a href="OT_Shape_Correspondence.pdf"><font color=blue size=+0.3>1</font></a>) was accepted to Conference on Computer Vision and Pattern Recognition (CVPR), 2024</p>
</li>
<li><p>[01/2024] I am glad to serve as an Area Chair of the International Conference on Machine Learning (ICML), 2024</p>
</li>
<li><p>[01/2024] Six papers (<a href="https://openreview.net/forum?id=jvtmdK69KQ"><font color=blue size=+0.3>1</font></a>, <a href="https://openreview.net/forum?id=Wd47f7HEXg"><font color=blue size=+0.3>2</font></a>, <a href="https://openreview.net/forum?id=4zZFGliCl9"><font color=blue size=+0.3>3</font></a>, <a href="https://openreview.net/forum?id=StYc4hQAEi"><font color=blue size=+0.3>4</font></a>, <a href="https://openreview.net/forum?id=gxhRR8vUQb"><font color=blue size=+0.3>5</font></a>, <a href="https://openreview.net/forum?id=l60EM8md3t"><font color=blue size=+0.3>6</font></a>) were accepted to International Conference on Learning Representations (ICLR), 2024</p>
</li>
<li><p>[01/2024] Two papers (<a href="https://arxiv.org/pdf/2305.07572.pdf"><font color=blue size=+0.3>1</font></a>, <a href="https://openreview.net/forum?id=Wd47f7HEXg"><font color=blue size=+0.3>2</font></a>) were accepted to International Conference on Artificial Intelligence and Statistics (AISTATS), 2024</p>
</li>
<li><p>[12/2023] The paper "<a href="SDE_Arxiv_submission_v2.pdf"><font color=blue size=+0.3> A diffusion process perspective on the posterior contraction rates for parameters </font></a>", coauthored with Wenlong Mou, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan was accepted to SIAM Journal on Mathematics of Data Science (SIMODS)</p>
</li>	
<li><p>[09/2023] Six papers (<a href="Softmax_MoE.pdf"><font color=blue size=+0.3>1</font></a>, <a href="Energy_sliced.pdf"><font color=blue size=+0.3>2</font></a>, <a href="https://arxiv.org/pdf/2301.11808.pdf"><font color=blue size=+0.3>3</font></a>, <a href="https://arxiv.org/pdf/2301.03749.pdf"><font color=blue size=+0.3>4</font></a>, <a href="https://arxiv.org/pdf/2210.05794.pdf"><font color=blue size=+0.3>5</font></a>, <a href="https://arxiv.org/pdf/2306.11925.pdf"><font color=blue size=+0.3>6</font></a>) were accepted to Conference on Neural Information Processing Systems (NeurIPS), 2023</p>
</li>	
<li><p>[08/2023] I am glad to serve as an Area Chair of the International Conference on Artificial Intelligence and Statistics (AISTATS), 2024</p>
</li>
<li><p>[04/2023] Four papers (<a href="https://arxiv.org/pdf/2301.11496.pdf"><font color=blue size=+0.3>1</font></a>, <a href="https://arxiv.org/pdf/2301.00437.pdf"><font color=blue size=+0.3>2</font></a>, <a href="https://arxiv.org/pdf/2211.15779.pdf"><font color=blue size=+0.3>3</font></a>, <a href="https://arxiv.org/pdf/2301.04791.pdf"><font color=blue size=+0.3>4</font></a>) were accepted to International Conference on Machine Learning (ICML), 2023</p>
</li>
<li><p>[04/2023] The paper "<a href="https://arxiv.org/pdf/1811.02657.pdf"><font color=blue size=+0.3> A Bayesian perspective of convolutional neural networks through a deconvolutional generative model </font></a>" , coauthored with Tan Nguyen, Ankit Patel, Richard Baraniuk, Anima Anandkumar, Michael I. Jordan was accepted to Journal of Machine Learning Research (JMLR)</p>
</li>
<li><p>[01/2023] I am glad to serve as an Associate Editor of the <a href="https://imstat.org/journals-and-publications/electronic-journal-of-statistics/"><font color=blue size=+0.3> Electronic Journal of Statistics </font></a>, a top journal in Statistics and Data Science</p>
</li>
<li><p>[01/2023] Three papers (<a href="Hierarchical_SW.pdf"><font color=blue size=+0.3>1</font></a>, <a href="https://openreview.net/forum?id=U_T8-5hClV"><font color=blue size=+0.3>2</font></a>, <a href="GLOT_arxiv.pdf"><font color=blue size=+0.3>3</font></a>) are accepted to International Conference on Learning Representations (ICLR) and International Conference on Artificial Intelligence and Statistics (AISTATS)</p>
</li>
<li><p>[11/2022] The paper "<a href="https://arxiv.org/pdf/2005.11411.pdf"><font color=blue size=+0.3> Instability, computational efficiency, and statistical accuracy </font></a>" , coauthored with Raaz Dwivedi, Koulik Khamaru, Martin J. Wainwright, Michael I. Jordan, and Bin Yu was accepted to Journal of Machine Learning Research (JMLR) subject to minor revision</p>
</li>
<li><p>[09/2022] Six papers (<a href="Revisiting_Sliced_Wasserstein_Arxiv.pdf"><font color=blue size=+0.3>1</font></a>, <a href="Fourier_former.pdf"><font color=blue size=+0.3>2</font></a>, <a href="Amortized_Projection_Optimization_for_Mini_batch_Projected_Wasserstein.pdf"><font color=blue size=+0.3>3</font></a>, <a href="https://tanmnguyen89.github.io/FiSHformer.pdf"><font color=blue size=+0.3>4</font></a>, <a href="v_arxiv.pdf"><font color=blue size=+0.3>5</font></a>, <a href="https://arxiv.org/pdf/2206.01934.pdf"><font color=blue size=+0.3>6</font></a>) were accepted to Conference on Neural Information Processing Systems (NeurIPS), 2022</p>
</li>
<li><p>[09/2022] The paper "<a href="https://www3.stat.sinica.edu.tw/ss_newpaper/SS-2022-0062_na.pdf"><font color=blue size=+0.3> Bayesian consistency with the supremum metric</font></a>", coauthored with Stephen Walker, was accepted to Statistica Sinica</p>
</li>
<li><p>[08/2022] I am glad to serve as an Area Chair of the International Conference on Artificial Intelligence and Statistics (AISTATS), 2023</p>
</li>
<li><p>[08/2022] The paper "<a href="https://arxiv.org/pdf/1907.04377.pdf"><font color=blue size=+0.3> Convergence rates for Gaussian mixtures of experts </font></a>" , coauthored with Chiao-Yu Yang and Michael I. Jordan, was accepted to Journal of Machine Learning Research (JMLR)</p>
</li>
<li><p>[05/2022] Six papers (<a href="Refined_metric.pdf"><font color=blue size=+0.3>1</font></a>, <a href="https://arxiv.org/pdf/2110.08678.pdf"><font color=blue size=+0.3>2</font></a>, <a href="https://arxiv.org/pdf/2108.10961.pdf"><font color=blue size=+0.3>3</font></a>, <a href="https://arxiv.org/pdf/2108.09645.pdf"><font color=blue size=+0.3>4</font></a>, <a href="https://arxiv.org/pdf/2102.05912.pdf"><font color=blue size=+0.3>5</font></a>, <a href="Architecture_Agnostic.pdf"><font color=blue size=+0.3>6</font></a>) were accepted to International Conference on Machine Learning (ICML), 2022</p>
</li>
<li><p>[05/2022] The paper "<a href="https://arxiv.org/pdf/1906.01437.pdf"><font color=blue size=+0.3> On the efficiency of entropic regularized
algorithms for optimal transport </font></a>" ,coauthored with Tianyi Lin and Michael I. Jordan, was accepted to Journal of Machine Learning Research (JMLR), 2022</p>
</li>
<li><p>[02/2022] The paper "<a href="https://arxiv.org/pdf/1910.00152.pdf"><font color=blue size=+0.3> On the complexity of approximating multi-marginal optimal transport</font></a>" ,coauthored with Tianyi Lin, Marco Cuturi, and Michael I. Jordan, was accepted to Journal of Machine Learning Research (JMLR), 2022</p>
</li>
<li><p>[01/2022] Four papers (<a href="Polyak_Step_Size_V3.pdf"><font color=blue size=+0.3>1</font></a>, <a href="https://arxiv.org/pdf/2108.07992.pdf"><font color=blue size=+0.3>2</font></a>, <a href="Trend_Filtering_AISTATS.pdf"><font color=blue size=+0.3>3</font></a>, <a href="Weak_Separation_AISTATS.pdf"><font color=blue size=+0.3>4</font></a>) were accepted to International Conference on Artificial Intelligence and Statistics (AISTATS), 2022</p>
</li>
</ul>

<h2 style="color:steelblue;"> Selected Publications on Theory (Hierarchical and Mixture Models, Bayesian Nonparametrics, Optimal Transport, Deep Learning, (Approximate) Bayesian Inference, (Non)-Convex Optimization, etc.)</h2>
<p>(<font color=red size=-0.3><b>* = equal contribution </b></font>)
<br /> 
(<font color=red size=-0.3><b>** = alphabetical order </b></font>)
<br />
(<font color=red size=-0.3><b><span>&dagger;</span> = co-last author </b></font>)
<br /></p>
<hr style="margin-bottom:0cm;">
<ul>
<li><p><a href="https://arxiv.org/pdf/2005.11411.pdf"><font color=blue size=+0.3> Instability, computational efficiency, and statistical accuracy </font></a>. <i>Accepted with 
minor revision at Journal of Machine Learning Research (JMLR), 2023</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2305.03288.pdf"><font color=blue size=+0.3> Demystifying softmax gating in Gaussian mixture of
experts </font></a>. <i> Advances in NeurIPS, 2023 (Spotlight)</i>. 
<br />
Huy Nguyen, Tin Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2401.13875.pdf"><font color=blue size=+0.3> Is temperature sample efficient for softmax Gaussian mixture of experts? </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Huy Nguyen, Pedram Akbarian, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2401.15771"><font color=blue size=+0.3> Bayesian nonparametrics meets data-driven robust optimization </font></a>. <i> Advances in NeurIPS, 2024</i>. 
<br />
Nicola Bariletto, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Sigmoid_MoE.pdf"><font color=blue size=+0.3> Sigmoid gating is more sample efficient than softmax gating in mixture of experts </font></a>. <i> Advances in NeurIPS, 2024</i>. 
<br />
Huy Nguyen, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Alessandro Rinaldo<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.02935"><font color=blue size=+0.3> On expert estimation in hierarchical mixture of experts: Beyond softmax gating functions </font></a>. <i> Under review</i>. 
<br />
Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Xing Han<font color=red size=-0.3><b>*</b></font>, Carl Harris, Suchi Saria, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.11222"><font color=blue size=+0.3> Quadratic gating functions in mixture of experts: A statistical insight </font></a>. <i> Under review</i>. 
<br />
Pedram Akbarian<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Xing Han<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.02200"><font color=blue size=+0.3> Revisiting prefix-tuning: Statistical benefits of reparameterization among prompts </font></a>. <i> ICLR, 2025</i>. 
<br />
Minh Le<font color=red size=-0.3><b>*</b></font>, Chau Nguyen<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Quyen Tran, Trung Le, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.08117"><font color=blue size=+0.3> On barycenter computation: Semi-unbalanced optimal transport-based method on Gaussians </font></a>. <i> Under review</i>. 
<br />
Hai Nguyen<font color=red size=-0.3><b>*</b></font>, Dung Le<font color=red size=-0.3><b>*</b></font>, Phi Nguyen, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.12258"><font color=blue size=+0.3> Understanding expert structures on minimax parameter estimation in contaminated mixture of experts </font></a>. <i> AISTATS, 2025</i>. 
<br />
Fanqi Yan<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Dung Le<font color=red size=-0.3><b>*</b></font>, Pedram Akbarian, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="HDP_optimization.pdf"><font color=blue size=+0.3> Borrowing strength in distributionally robust optimization via hierarchical Dirichlet processes </font></a>. <i> Under review</i>. 
<br />
Nicola Bariletto, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Cosine_MoE_v2.pdf"><font color=blue size=+0.3> 	
Statistical advantages of perturbing cosine router in sparse mixture of experts </font></a>. <i> ICLR, 2025</i>. 
<br />
Huy Nguyen, Pedram Akbarian<font color=red size=-0.3><b>*</b></font>, Trang Pham<font color=red size=-0.3><b>*</b></font>, Trang Nguyen<font color=red size=-0.3><b>*</b></font>, Shujian Zhang <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2402.02952"><font color=blue size=+0.3> On least square estimation in softmax gating mixture of experts </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Huy Nguyen, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Alessandro Rinaldo<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/forum?id=jvtmdK69KQ"><font color=blue size=+0.3> Statistical perspective of top-K sparse softmax gating mixture of experts </font></a>. <i> ICLR, 2024</i>. 
<br />
Huy Nguyen, Pedram Akbarian, Fanqi Yan, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="JMLR_V1.pdf"><font color=blue size=+0.3> Multivariate smoothing via the Fourier integral theorem and Fourier kernel </font></a>. <i> Under revision</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="SDE_Arxiv_submission_v2.pdf"><font color=blue size=+0.3> A diffusion process perspective on the posterior contraction rates for parameters</font></a>. <i> SIAM Journal on Mathematics of Data Science (SIMODS), 2023</i>.
<br />
Wenlong Mou, <b>Nhat Ho</b>, Martin J. Wainwright, Peter L. Bartlett, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2305.07572.pdf"><font color=blue size=+0.3> Towards convergence rates for parameter estimation
in Gaussian-gated mixture of experts </font></a>. <i> AISTATS, 2024</i>. 
<br />
Huy Nguyen, Tin Nguyen, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2301.11808.pdf"><font color=blue size=+0.3> Minimax optimal rate for parameter estimation in multivariate deviated models</font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Dat Do<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2401.02058"><font color=blue size=+0.3> Neural collapse for cross-entropy class-imbalanced learning with unconstrained ReLU feature model </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Hien Dang, Tho Tran, Tan Nguyen<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2309.13850.pdf"><font color=blue size=+0.3> A general theory for softmax gating multinomial logistic mixture of experts </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Huy Nguyen, Pedram Akbarian, Tin Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2301.00437.pdf"><font color=blue size=+0.3> Neural collapse in deep linear network: from balanced to imbalanced data </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Hien Dang<font color=red size=-0.3><b>*</b></font>, Tho Tran<font color=red size=-0.3><b>*</b></font>, Hung Tran, Tan Nguyen<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/forum?id=4zZFGliCl9"><font color=blue size=+0.3> Beyond vanilla variational autoencoders: Detecting posterior collapse in conditional and hierarchical variational autoencoders </font></a>. <i> ICLR, 2024</i>. 
<br />
Hien Dang, Tho Tran, Tan Nguyen<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="Bayesian_Consistency_Smooth_Final.pdf"><font color=blue size=+0.3> Bayesian consistency with the supremum metric </font></a>. <i> Statistica Sinica, 2022</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="Short_Arxiv_version.pdf"><font color=blue size=+0.3> An exponentially increasing step-size for parameter estimation in statistical models</font></a>. <i> Under review</i>. 
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Tongzheng Ren<font color=red size=-0.3><b>**</b></font>, Purnamrita Sarkar<font color=red size=-0.3><b>**</b></font>, Sujay Sanghavi<font color=red size=-0.3><b>**</b></font>, Rachel Ward<font color=red size=-0.3><b>**</b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="main_version.pdf"><font color=blue size=+0.3> On integral theorems and their statistical properties </font></a>. <i> Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 2024</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2301.11496.pdf"><font color=blue size=+0.3> On excess mass behavior in Gaussian mixture models with Orlicz Wasserstein distances </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Aritra Guha, <b>Nhat Ho</b>, Long Nguyen.</p>
</li>
</ul>
<ul>
<li><p><a href="v_arxiv.pdf"><font color=blue size=+0.3> Beyond black box densities: Parameter learning for the deviated components</font></a>. <i>Advances in NeurIPS, 2022</i>.
<br />
Dat Do<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Long Nguyen.</p>
</li>
</ul>
<ul>
<li><p><a href="Refined_metric.pdf"><font color=blue size=+0.3> Refined convergence rates for maximum likelihood estimation under finite mixture models</font></a>. <i> Proceedings of the ICML, 2022 (Long Presentation)</i>. 
<br />
Tudor Manole, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2108.10961.pdf"><font color=blue size=+0.3>  Entropic Gromov-Wasserstein between Gaussian distributions</font></a>. <i>Proceedings of the ICML, 2022</i>. 
<br />
Khang Le<font color=red size=-0.3><b>*</b></font>, Dung Le<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Dat Do, Tung Pham, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Polyak_Step_Size_V3.pdf"><font color=blue size=+0.3> Towards statistical and computational complexities of Polyak step size gradient descent </font></a>. <i>AISTATS, 2022</i>. 
<br />
Tongzheng Ren<font color=red size=-0.3><b>*</b></font>, Fuheng Cui<font color=red size=-0.3><b>*</b></font>, Alexia Atsidakou<font color=red size=-0.3><b>*</b></font>, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.02601.pdf"><font color=blue size=+0.3> On the minimax optimality of the EM algorithm for learning two-component mixed linear regression</font></a>. <i>AISTATS, 2021</i>. 
<br />
Jeong Y. Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p><a href="Linear_scale_location.pdf"><font color=blue size=+0.3> Beyond EM algorithm on over-specified two-component location-scale Gaussian mixtures</font></a>. <i> Under review</i>. 
<br />
Tongzheng Ren<font color=red size=-0.3><b>*</b></font>, Fuheng Cui<font color=red size=-0.3><b>*</b></font>, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1906.01437.pdf"><font color=blue size=+0.3> On the efficiency of entropic regularized
algorithms for optimal transport </font></a>. <i> Journal of Machine Learning Research (JMLR), 2022</i>. 
<br />
Tianyi Lin, <b>Nhat Ho</b>, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="Mixture_Experts_Arxiv.pdf"><font color=blue size=+0.3> Convergence rates for Gaussian mixtures of experts</font></a>. <i>Journal of Machine Learning Research (JMLR), 2022</i>.
<br />
<b>Nhat Ho</b>, Chiao-Yu Yang, Michael I. Jordan.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2102.02756.pdf"><font color=blue size=+0.3> On the computational and statistical complexity of over-parameterized matrix sensing </font></a>. <i>Journal of Machine Learning Research (JMLR), 2024</i>.
<br />
Jiacheng Zhuo, Jeongyeol Kwon, <b>Nhat Ho</b>, Constantine Caramanis.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.07458.pdf"><font color=blue size=+0.3> Projection robust Wasserstein distance and Riemannian optimization </font></a>. <i> Advances in NeurIPS, 2020 (Spotlight)</i>. 
<br />
Tianyi Lin<font color=red size=-0.3><b>*</b></font>, Chenyou Fan<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>, Marco Cuturi, Michael I. Jordan. </p>
</li>
</ul>
<ul>
<li><p><a href="Normalized_GD.pdf"<font color=blue size=+0.3> Improving computational complexity in statistical models with second-order information </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Tongzheng Ren, Jiacheng Zhuo, Sujay Sanghavi, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1901.05078.pdf"><font color=blue size=+0.3> On posterior contraction of parameters and interpretability in Bayesian mixture modeling </font></a>. <i> Bernoulli 27 (4), 2159-2188, 2021</i>.
<br />
Aritra Guha, <b>Nhat Ho</b>, XuanLong Nguyen.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1810.00828.pdf"><font color=blue size=+0.3> Singularity,
misspecification, and the convergence rate of EM</font></a>. <i>Annals of Statistics, 48(6), 3161-3182, 2020</i>. 
<br />
Raaz Dwivedi<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Koulik Khamaru<font color=red size=-0.3><b>*</b></font>, Martin J. Wainwright, Michael I. Jordan, Bin Yu.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1609.02655.pdf"><font color=blue size=+0.3> Singularity structures and impacts on parameter estimation behavior in finite mixtures of distributions</font></a>. <i>SIAM Journal on Mathematics of Data Science (SIMODS), 1(4), 730–758, 2019</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen.</p>
</li>
</ul>
<ul>
<li><p><a href="AoS_2016.pdf"><font color=blue size=+0.3>Convergence rates of parameter estimation for some weakly identifiable finite mixtures</font></a>. <i>Annals of Statistics, 44(6), 2726-2755, 2016</i>.
<br />
<b>Nhat Ho</b> and XuanLong Nguyen</p>
</li>
</ul>
<h2 style="color:steelblue;"> Selected Publications on Method and Application (Optimal Transport, Transformer, Deep Generative Models,  3D Deep Learning, Convolutional Neural Networks, etc.)</h2>
<hr style="margin-bottom:0cm;">
<ul>
<li><p><a href="https://arxiv.org/pdf/1811.02657.pdf"><font color=blue size=+0.3> A Bayesian perspective of convolutional neural networks through a deconvolutional generative model</font></a>. <i>Journal of Machine Learning Research (JMLR), 2023</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Ankit Patel, Anima Anandkumar, Michael I. Jordan, Richard Baraniuk. </p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/forum?id=Wd47f7HEXg"><font color=blue size=+0.3> Quasi-Monte Carlo for 3D sliced Wasserstein </font></a>. <i> ICLR, 2024 (Spotlight)</i>. 
<br />
Khai Nguyen, Nicola Bariletto, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/forum?id=StYc4hQAEi"><font color=blue size=+0.3> Sliced Wasserstein estimation with control variates </font></a>. <i> ICLR, 2024</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2401.15889"><font color=blue size=+0.3> Sliced Wasserstein with random-path projecting directions </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Khai Nguyen, Shujian Zhang, Tam Le, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2402.03226.pdf"><font color=blue size=+0.3> FuseMoE: Mixture-of-experts Transformers for fleximodal fusion </font></a>. <i> Advances in NeurIPS, 2024</i>. 
<br />
Xing Han, Huy Nguyen, Carl William Harris,  <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Suchi Saria<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="Hierarchical_Hybrid_Sliced_Wasserstein.pdf"><font color=blue size=+0.3> Hierarchical hybrid sliced Wasserstein: A scalable metric for heterogeneous joint distributions </font></a>. <i> Advances in NeurIPS, 2024</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2402.02526"><font color=blue size=+0.3> CompeteSMoE - Effective training of sparse mixture of experts via competition </font></a>. <i> Under review</i>. 
<br />
Quang Pham, Truong Giang Do, Huy Nguyen, TrungTin Nguyen, Chenghao Liu, Mina Sartipi, Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, <b>Steven Hoi</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Nhat Ho<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="MoE_Continual_Learning.pdf"><font color=blue size=+0.3> 	
Mixture of experts meets prompt-based continual learning </font></a>. <i> Advances in NeurIPS, 2024</i>. 
<br />
Minh Le, An Nguyen<font color=red size=-0.3><b>*</b></font>, Huy Nguyen<font color=red size=-0.3><b>*</b></font>, Trang Nguyen<font color=red size=-0.3><b>*</b></font>, Trang Pham<font color=red size=-0.3><b>*</b></font>, Linh Van Ngo, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2405.07482"><font color=blue size=+0.3> Marginal fairness sliced Wasserstein barycenter </font></a>. <i> ICLR, 2025</i>. 
<br />
Khai Nguyen, Hai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.02615"><font color=blue size=+0.3> LoGra-Med: Long context multi-graph alignment for medical vision-language model </font></a>. <i> Under review</i>. 
<br />
Duy M. H. Nguyen, Nghiem T. Diep, Trung Q. Nguyen, Hoang-Bao Le, Tai Nguyen, Tien Nguyen, TrungTin Nguyen, <b>Nhat Ho</b>, Pengtao Xie, Roger Wattenhofer, James Zhou, Daniel Sonntag, Mathias Niepert.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.12258"><font color=blue size=+0.3> X-Drive: Cross-modality consistent multi-sensor data synthesis for driving scenarios </font></a>. <i> ICLR, 2025</i>. 
<br />
Yichen Xie, Chenfeng Xu, Chensheng Peng, Shuqi Zhao, <b>Nhat Ho</b>, Alexander T. Pham, Mingyu Ding, Wei Zhan, Masayoshi Tomizuka.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.12258"><font color=blue size=+0.3> ACUS: Audio captioning with unbiased sliced Wasserstein kernel </font></a>. <i> Under review</i>. 
<br />
Manh Luong, Khai Nguyen, Dinh Phung, <b>Nhat Ho</b>, Reza Haf, Lizhen Qu.</p>
</li>
</ul>
<ul>
<li><p><a href="Backdoor_Continual_Learning_v2.pdf"><font color=blue size=+0.3> Backdoor attack in prompt-based continual learning </font></a>. <i> AAAI, 2025</i>. 
<br />
Trang Nguyen, Anh Tran, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2402.01975"><font color=blue size=+0.3> Structure-aware E(3)-invariant molecular conformer Aggregation Networks </font></a>. <i> Proceedings of the ICML, 2024</i>. 
<br />
Duy Minh Ho Nguyen, Nina Lukashina, Tai Nguyen, An Thai Le, TrungTin Nguyen, <b>Nhat Ho</b>, Jan Peters, Daniel Sonntag, Viktor Zaverkin, Mathias Niepert.</p>
</li>
</ul>
<ul>
<li><p><a href="OT_Shape_Correspondence.pdf"><font color=blue size=+0.3> Integrating efficient optimal transport and functional maps For unsupervised shape correspondence learning</font></a>. <i> Conference on Computer Vision and Pattern Recognition (CVPR), 2024</i>. 
<br />
Thanh Tung Le, Khai Nguyen, shanlin sun, <b>Nhat Ho</b>, Xiaohui Xie.</p>
</li>
</ul>
<ul>
<li><p><a href="Energy_sliced.pdf"><font color=blue size=+0.3> Energy-based sliced Wasserstein distance </font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2301.03749.pdf"><font color=blue size=+0.3> Markovian sliced Wasserstein distances: Beyond
independent projections </font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Khai Nguyen, Tongzheng Ren, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Revisiting_Sliced_Wasserstein_Arxiv.pdf"><font color=blue size=+0.3> Revisiting projected Wasserstein metric on images: from vectorization to convolution</font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Fourier_former.pdf"><font color=blue size=+0.3> FourierFormer: Transformer meets generalized Fourier integral attentions </font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Minh Pham<font color=red size=-0.3><b>*</b></font>, Tam Nguyen, Khai Nguyen, Stanley Osher, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2110.08678.pdf"<font color=blue size=+0.3> Improving Transformers with probabilistic attention keys </font></a>. <i>ICML, 2022</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Dung Le, Khuong Nguyen, Anh Tran, Richard Baraniuk, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="Amortized_Projection_Optimization_for_Mini_batch_Projected_Wasserstein.pdf"><font color=blue size=+0.3> Amortized projection optimization for sliced Wasserstein generative models</font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Khai Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="Hierarchical_SW.pdf"><font color=blue size=+0.3> Hierarchical sliced Wasserstein distance</font></a>. <i> ICLR, 2023</i>. 
<br />
Khai Nguyen, Tongzheng Ren, Huy Nguyen, Litu Rout, Tan Nguyen, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2210.05794.pdf"><font color=blue size=+0.3> Designing robust transformers using robust kernel density estimation</font></a>. <i> Advances in NeurIPS, 2023</i>. 
<br />
Xing Han, Tongzheng Ren, Tan Minh Nguyen, Khai Nguyen, Joydeep Ghosh, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="ICLR_Primal_Dual.pdf"><font color=blue size=+0.3> A primal-dual framework for transformers and neural networks</font></a>. <i> ICLR, 2023 (Spotlight)</i>. 
<br />
Tan Minh Nguyen, Tam Minh Nguyen, <b>Nhat Ho</b>, Andrea L. Bertozzi, Richard Baraniuk, Stanley Osher.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2301.04791.pdf"><font color=blue size=+0.3> Self-attention amortized distributional projection optimization for sliced
Wasserstein point-clouds reconstruction </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Khai Nguyen<font color=red size=-0.3><b>*</b></font>, Dang Nguyen<font color=red size=-0.3><b>*</b></font>, <b>Nhat Ho</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2211.15779.pdf"><font color=blue size=+0.3> Revisiting over-smoothing and over-squashing using Ollivier's Ricci curvature </font></a>. <i> Proceedings of the ICML, 2023</i>. 
<br />
Khang Nguyen, Tan Nguyen, Nong Hieu, Vinh Nguyen, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>, Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="ICML_FiSHformer.pdf"><font color=blue size=+0.3>  Improving Transformer with an admixture of attention heads </font></a>. <i> Advances in NeurIPS, 2022</i>. 
<br />
Tam Nguyen<font color=red size=-0.3><b>*</b></font>, Tan Nguyen<font color=red size=-0.3><b>*</b></font>, Hai Do, Khai Nguyen, Vishwanath Saragadam, Minh Pham, Khuong Nguyen,Stanley Osher<font color=red size=-0.3><b><span>&dagger;</span></b></font>, <b>Nhat Ho</b><font color=red size=-0.3><b><span>&dagger;</span></b></font>.</p>
</li>
</ul>
<ul>
<li><p><a href="Statistical_Fourier_Theorem.pdf"><font color=blue size=+0.3> Generative models from the multivariate Fourier integral theorem </font></a>. <i> Under review</i>.
<br />
<b>Nhat Ho</b><font color=red size=-0.3><b>**</b></font>, Stephen G. Walker<font color=red size=-0.3><b>**</b></font>.  </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.06448.pdf"><font color=blue size=+0.3> Probabilistic best subset selection via gradient-based optimization</font></a>. <i>Under review</i>. 
<br />
Mingzhang Yin, <b>Nhat Ho</b>, Bowei Yan, Xiaoning Qian, Mingyuan Zhou.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2020-08-19 03:32:40 CDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
